{"0": {
    "doc": "üìÜ Calendar",
    "title": "üìÜ Calendar",
    "content": "Coming soon üëÄ . ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"1": {
    "doc": "Project 3",
    "title": "Project 3 ‚Äì Exploratory Data Analysis üìä",
    "content": "Due Date: Thursday, February 23rd at 11:59PM (No Checkpoint!) . ",
    "url": "/project3/#project-3--exploratory-data-analysis-",
    "relUrl": "/project3/#project-3--exploratory-data-analysis-"
  },"2": {
    "doc": "Project 3",
    "title": "Table of Contents",
    "content": ". | Overview | Choosing a Dataset | Part 1: Analysis . | Requirement: Introduction | Requirement: Cleaning and EDA (Exploratory Data Analysis) | Requirement: Assessment of Missingness | Requirement: Hypothesis Testing | Style | . | Part 2: Report . | Step 1: Initializing a Jekyll GitHub Pages Site | Step 2: Choosing a Theme | Step 3: Embedding Content | Local Setup | . | Submission and Rubric . | Submission | Rubric | . | . ",
    "url": "/project3/#table-of-contents",
    "relUrl": "/project3/#table-of-contents"
  },"3": {
    "doc": "Project 3",
    "title": "Overview",
    "content": "Welcome to Project 3! üëã . This project contains no new material. Rather, it‚Äôs a good opportunity to sharpen your understanding of the core concepts of the first half of the course. It‚Äôll also give you practice with creating visualizations and websites, and will give you something concrete to put on your resume and show to potential employers! . The project is broken into two parts: . | Part 1: An analysis, submitted as a Jupyter Notebook. This will contain the details of your work. Focus on completing your analysis before moving to Part 2, as the analysis is the bulk of the project. | Part 2: A report, submitted as a website. This will contain a narrative ‚Äústory‚Äù with visuals. Focus on this after finishing most of your analysis. | . The project is due on Thursday, February 23rd at 11:59PM. While there is no checkpoint, we encourage you to finish Part 1 by Monday, February 20th, to leave yourself three days to review your analysis and prepare your report.Like other projects, you‚Äôre welcome to work with a partner, though if you do, you must both work on all pieces together, simultaneously. You will only submit one notebook and create one website. ",
    "url": "/project3/#overview",
    "relUrl": "/project3/#overview"
  },"4": {
    "doc": "Project 3",
    "title": "Choosing a Dataset",
    "content": "In this project, you will perform an open-ended investigation into a single dataset (that‚Äôs right ‚Äì no autograders!). You must choose one of the following three datasets: . | Recipes and Ratings üçΩ | Power Outages üîã | League of Legends Competitive Matches ‚å®Ô∏è | . By clicking the above links, you‚Äôll find descriptions, download links, and guidelines for each dataset. When selecting which dataset you are going to use for your project, try choosing the one whose topic appeals to you the most as that will make finishing the project a lot more enjoyable. Furthermore, Project 5 will also be an open-ended project and will require you to choose one of the same three datasets. We recommend that you choose the same dataset in both Project 3 and Project 5, so that you don‚Äôt have to relearn a new dataset for Project 5. You may want to read the rest of this page before choosing a dataset. ",
    "url": "/project3/#choosing-a-dataset",
    "relUrl": "/project3/#choosing-a-dataset"
  },"5": {
    "doc": "Project 3",
    "title": "Part 1: Analysis",
    "content": "Before beginning your analysis, you‚Äôll need to set up a few things. | Pull the latest version of the dsc80-2023-wi repo. Within the projects/03-eda folder, there is a template.ipynb notebook that you will use as a template for the project. If you delete the file or want another copy of the template, you can re-download it from here. This is where your analysis will live; you will submit this entire notebook to us. | Select one of the three datasets mentioned above, download it, and load it into your template notebook. | . Once you have your dataset loaded in your notebook, it‚Äôs time for you to find meaning in the real-world data you‚Äôve collected! Follow the steps below. Tip: For each step, we specify what must be done in your notebook and what must go on your website. We recommend you write everything in your notebook first, and then move things over to your website once you‚Äôve completed your analysis. Requirement: Introduction . | Step | Analysis in Notebook | Report on Website | . | Introduction and Question Identification | Understand the data you have access to. Brainstorm a few questions that interest you about the dataset. Pick one question you plan to investigate further. (As the data science lifecycle tells us, this question may change as you work on your project.) | Provide an introduction to your dataset, and clearly state the one question your analysis is centered around. Why should readers of your website care about the dataset and your question specifically? Report the number of rows in the dataset, the names of the columns that are relevant to your question, and descriptions of those relevant columns. | . Requirement: Cleaning and EDA (Exploratory Data Analysis) . | Step | Analysis in Notebook | Report on Website | . | Data Cleaning | Clean the data appropriately. For instance, you may need to replace data that should be missing with NaN or create new columns out of given ones (e.g. compute distances, scale data, or get time information from time stamps). | Describe, in detail, the data cleaning steps you took and how they affected your analyses. The steps should be explained in reference to the data generating process. Show the head of your cleaned DataFrame (see Part 2: Report for instructions). | . | Univariate Analysis | Look at the distributions of relevant columns separately by using DataFrame operations and drawing at least two relevant plots. | Embed at least one plotly plot you created in your notebook that displays the distribution of a single column (see Part 2: Report for instructions). Include a 1-2 sentence explanation about your plot, making sure to describe and interpret any trends present. (Your notebook will likely have more visualizations than your website, and that‚Äôs fine. Feel free to embed more than one univariate visualization in your website if you‚Äôd like, but make sure that each embedded plot is accompanied by a description.) | . | Bivariate Analysis | Look at the statistics of pairs of columns to identify possible associations. For instance, you may create scatter plots and plot conditional distributions, or box-plots. You must plot at least two such plots in your notebook. The results of your bivariate analyses will be helpful in identifying interesting hypothesis tests! | Embed at least one plotly plot that displays the relationship between two columns. Include a 1-2 sentence explanation about your plot, making sure to describe and interpret any trends present. (Your notebook will likely have more visualizations than your website, and that‚Äôs fine. Feel free to embed more than one bivariate visualization in your website if you‚Äôd like, but make sure that each embedded plot is accompanied by a description.) | . | Interesting Aggregates | Choose columns to group and pivot by and examine aggregate statistics. | Embed at least one grouped table or pivot table in your website and explain its significance. | . Requirement: Assessment of Missingness . | Step | Analysis in Notebook | Report on Website | . | NMAR Analysis | Recall, to determine whether data are likely NMAR, you must reason about the data generating process; you cannot conclude that data are likely NMAR solely by looking at your data. As such, there‚Äôs no code to write here (and hence, nothing to put in your notebook). | State whether you believe there is a column in your dataset that is NMAR. Explain your reasoning and any additional data you might want to obtain that could explain the missingness (thereby making it MAR). Make sure to explicitly use the term ‚ÄúNMAR.‚Äù | . | Missingness Dependency | Pick a column in the dataset with non-trivial missingness to analyze, and perform permutation tests to analyze the dependency of the missingness of this column on other columns.Specifically, find at least one other column that the missingness of your selected column does depend on, and at least one other column that the missingness of your selected column does not depend on.Tip: Make sure you know the difference between the different types of missingness before approaching that section. Many students in the past have lost credit for mistaking one type of missingness for another. | Present and interpret the results of your missingness permutation tests with respect to your data and question. Embed a plotly plot related to your missingness exploration; ideas include:‚Ä¢ The distribution of column \\(Y\\) when column \\(X\\) is missing and the distribution of column \\(Y\\) when column \\(X\\) is not missing, as was done in Lecture 12.‚Ä¢ The empirical distribution of the test statistic used in one of your permutation tests, along with the observed statistic. | . Requirement: Hypothesis Testing . | Step | Analysis in Notebook | Report on Website | . | Hypothesis Testing | Clearly state a pair of hypotheses and perform a hypothesis test or permutation test that is not related to missingness. Feel free to use the ‚Äúsample questions‚Äù in each of the dataset descriptions or create your own. This should be the question that is stated clearly at the top of your report. | Clearly state your null and alternative hypotheses, your choice of test statistic and significance level, the resulting \\(p\\)-value, and your conclusion. Justify why these choices are good choices for answering the question you are trying to answer.Optional: Embed a visualization related to your hypothesis test in your website.Tip: When making writing your conclusions to the statistical tests in this project, never use language that implies an absolute conclusion; since we are performing statistical tests and not randomized controlled trials, we cannot prove that either hypothesis is 100% true or false.‚ÄúOnly a Sith deals in absolutes‚Äù - Obi-Wan Kenobi | . Style . While your website will neatly organized and tailored for public consumption, it is important to keep your analysis notebook organized as well. Follow these guidelines: . | Your work for each of the three project sections (Cleaning and EDA, Assessment of Missingness, and Hypothesis Testing) described above should be completed in code cells underneath the Markdown header of that section‚Äôs name. | You should only include work that is relevant to posing, explaining, and answering the question(s) you state in your report. You should include data quality, cleaning, and missingness assessments, though these should broadly be relevant to the question at hand. | Make sure to clearly explain what each component of your notebook means. Specifically: . | All plots should have titles, labels, and a legend (if applicable), even if they don‚Äôt make it into your website. Plots should be self-contained ‚Äì readers should be able to understand what they describe without having to read anything else. | All code cells should contain a comment describing how the code works (unless the code is self-explanatory ‚Äì use your best judgement). | . | . ",
    "url": "/project3/#part-1-analysis",
    "relUrl": "/project3/#part-1-analysis"
  },"6": {
    "doc": "Project 3",
    "title": "Part 2: Report",
    "content": "The purpose of your website is to provide the general public ‚Äì your classmates, friends, family, recruiters, and random internet strangers ‚Äì with an overview of your project and its findings, without forcing them to understand every last detail. We don‚Äôt expect the website creation process to take very much time, but it will certainly be rewarding. Once you‚Äôve completed your analysis and know what you will put in your website, start reading this section. Your website must clearly contain four headings: . | Introduction | Cleaning and EDA | Assessment of Missingness | Hypothesis Testing | . The specific content your website needs to contain is described in the ‚ÄúReport on Website‚Äù columns above. Make sure to also give your website a creative title that relates to the question you‚Äôre trying to answer, and clearly state your name(s). Your report will be in the form of a static website, hosted for free on GitHub Pages. More specifically, you‚Äôll use Jekyll, a framework built into GitHub Pages that allows you to create professional-looking websites just by writing Markdown (dsc80.com is built using Jekyll!). GitHub Pages does the ‚Äúhard‚Äù part of converting your Markdown to HTML. If you‚Äôd like to follow the official GitHub Pages &amp; Jekyll tutorial, you‚Äôre welcome to, though we will provide you with a perhaps simpler set of instructions here. A very basic site with the required headings and one embedded visualization can be found at rampure.org/dsc80-proj3-test1/; the source code for the site is here. Step 1: Initializing a Jekyll GitHub Pages Site . | Create a GitHub account, if you don‚Äôt already have one. | Create a new GitHub repository for your project. | GitHub Pages sites live at &lt;username&gt;.github.io/&lt;reponame&gt; (for instance, the site for github.com/dsc-courses/dsc80-2023-wi is dsc-courses.github.io/dsc80-2023-wi). | As such, don‚Äôt include ‚ÄúDSC 80‚Äù or ‚ÄúProject 3‚Äù in your repo‚Äôs name ‚Äì this looks unprofessional to future employers, and gives you a generic-sounding URL. Instead, mention that this is a project for DSC 80 at UCSD in the repository description. | Make sure to make your repository public. | Select ‚ÄúADD a README file.‚Äù This ensures that your repository starts off non-empty, which is necessary to continue. | . | Click ‚ÄúSettings‚Äù in the repository toolbar (next to ‚ÄúInsights‚Äù), then click ‚ÄúPages‚Äù in the left menu. | Under ‚ÄúBranch‚Äù, click the ‚ÄúNone‚Äù dropdown, change the branch to ‚Äúmain‚Äù, and then click ‚ÄúSave.‚Äù You should soon see ‚ÄúGitHub Pages source saved.‚Äù in a blue banner at the top of the page. This initiates GitHub Pages in your repository. | After 30 seconds, reload the page again. You should see ‚ÄúYour site is live at http://username.github.io/reponame/‚Äù. Click that link ‚Äì you now have a site! | Click ‚ÄúCode‚Äù in the repo toolbar to return to the source code for your repo. | . Note that the source code for your site lives in README.md. As you push changes to README.md, they will update on your site automatically within a few minutes! Before moving forward, make sure that you can make basic edits: . | Clone your repo locally. | Make some edits to README.md. | Push those changes back to GitHub, using the following steps: . | Add your changes to ‚Äústaging‚Äù using git add README.md (repeat this for any other files you add). | Commit your changes using git commit -m '&lt;some message here&gt;', e.g. git commit -m 'changed title of website'. | Push your changes using git push. | . | . Moving forward, we recommend making edits to your website source code locally, rather than directly on GitHub. This is in part due to the fact that you‚Äôll be creating folders and adding files to your source code. Step 2: Choosing a Theme . The default ‚Äútheme‚Äù of a Jekyll site is not all that interesting. To change the theme of your site: . | Create a file in your repository called _config.yml. | Go here, and click the links of various themes until you find one you like. | Open the linked repository for the theme you‚Äôd like to use and scroll down to the ‚ÄúUsage‚Äù section of the README. Copy the necessary information from the README to your _config.yml and push it to your site. | . For instance, if I wanted to use the Merlot theme, I‚Äôd put the following in my _config.yml: . remote_theme: pages-themes/merlot@v0.2.0 plugins: - jekyll-remote-theme # add this line to the plugins list if you already have one . Note that you‚Äôre free to use any Jekyll theme, not just the ones that appear here. You are required to choose some theme other than the default, though. See more details about themes here. Step 3: Embedding Content . Now comes the interesting part ‚Äì actually including content in your site. The Markdown cheat sheet contains tips on how to format text and other page components in Markdown (and if you‚Äôd benefit by seeing an example, you could always look at the Markdown source of this very page ‚Äì meta!). What will be a bit trickier is embedding plotly plots in your site so that they are interactive. Note that you are required to do this, you cannot simply take screenshots of plots from your notebooks and embed them in your site. Here‚Äôs how to embed a plotly plot directly in your site. | First, you‚Äôll need to convert your plot to HTML. If fig is a plotly Figure object (for instance, the result of calling px.express, go.Figure, or .plot when pd.options.plotting.backend = \"plotly\" has been run), then the method fig.write_html saves the plot as HTML to a file. Call it using fig.write_html('file-name.html', include_plotlyjs='cdn'). | Change 'file-name.html' to the path where you‚Äôd like to initially save your plot. | include_plotlyjs='cdn' tells write_html to load the source code for the plotly library from a server, rather than including the entire source code in your HTML file. This drastically reduces the size of the resulting HTML file, keeping your repo size down. | . | Move the .html file(s) you‚Äôve created into a folder in your website repo called assets (or something similar). | Depending on where your template notebook is saved, you could combine this step with the step above by calling fig.write_html with the correct path (e.g. `fig.write_html(‚Äò../league-match-analysis/assets/matches-per-year.html‚Äô)). | . | In README.md, embed your plot using the following syntax: | . &lt;iframe src=\"assets/file-name.html\" width=800 height=600 frameBorder=0&gt;&lt;/iframe&gt; . | iframe stands for ‚Äúinline frame‚Äù; it allows us to embed HTML files within other HTML files. | You can change the width and height arguments, but don‚Äôt change the frameBorder argument. | . Refer here for a working example. Try your best to make your plots look professional and unique to your group ‚Äì add axis labels, change the font and colors, add annotations, etc. Remember, potential employers will see this ‚Äì you don‚Äôt want your plots to look like everyone else‚Äôs! . To convert a DataFrame in your notebook to Markdown source code (which you need to do for both the Data Cleaning and Interesting Aggregates sections of the cleaning and EDA requirement), use the .to_markdown() method on the DataFrame. For instance, . print(counts[['Quarter', 'Count']].head().to_markdown(index=False)) . displays a string, containing the Markdown representation of the first 5 rows of counts, including only the 'Quarter' and 'Count' columns (and not including the index). You can see how this appears here; remember, no screenshots. Local Setup . The above instructions give you all you need to create and make updates to your site. However, you may want to set up Jekyll locally, so that you can look at how changes to your site would look without having to push and wait for GitHub to re-build your site. To do so, follow the steps here and then here. ",
    "url": "/project3/#part-2-report",
    "relUrl": "/project3/#part-2-report"
  },"7": {
    "doc": "Project 3",
    "title": "Submission and Rubric",
    "content": "Submission . You will submit your project in two ways: . | By uploading a PDF version of your notebook to the specific ‚ÄúProject 3 Notebook PDF (Dataset)‚Äù assignment on Gradescope for your dataset. | To export your notebook as a PDF, first, restart your kernel and run all cells. Then, go to ‚ÄúFile &gt; Print Preview‚Äù. Then, save a print preview of the webpage as a PDF. There are other ways to save a notebook as a PDF but they may require that you have additional packages installed on your computer, so this is likely the most straightforward. | It‚Äôs fine if your plotly graphs don‚Äôt render in the PDF output of your notebook. | This notebook asks you to include a link to your website; make sure to do so. | . | By submitting a link to your website to the ‚ÄúProject 3 Website Link (All Datasets)‚Äù assignment on Gradescope. | . To both submissions, make sure to tag your partner. You don‚Äôt need to submit your actual .ipynb file anywhere. While your website must be public and you should share it with others, you should not make your code for this project available publicly. Since there are two assignments you need to submit to on Gradescope, we will treat your submission time as being the latter of your two submissions. So, if you submit to the ‚ÄúProject 3 Notebook PDF‚Äù assignment before the deadline but to the ‚ÄúProject 3 Website Link (All Datasets)‚Äù website one day late, overall, you will be charged a slip day. There are a lot of moving parts to this assignment ‚Äì don‚Äôt wait until the last minute to try and submit! . Rubric . Your project will be graded out of 100 points. The rough rubric is shown below. If you satisfy these requirements as described above, you will receive full credit. | Component | Weight | . | Provided an introduction to the dataset and the analyses | 8 points | . | Cleaned data | 8 points | . | Performed univariate analyses | 8 points | . | Performed bivariate analyses and aggregation | 8 points | . | Addressed NMAR question | 4 points | . | Performed permutation tests for missingness | 8 points | . | Interpreted missingness test results | 8 points | . | Selected relevant columns for a hypothesis or permutation test | 4 points | . | Explicitly stated a null hypothesis | 4 points | . | Explicitly stated an alternative hypothesis | 4 points | . | Performed a hypothesis or permutation test | 8 points | . | Used a valid test statistic | 4 points | . | Computed a p-value and made a decision | 4 points | . | Included all necessary components on the website | 20 points | . | Total | 100 points | . ",
    "url": "/project3/#submission-and-rubric",
    "relUrl": "/project3/#submission-and-rubric"
  },"8": {
    "doc": "Project 3",
    "title": "Project 3",
    "content": " ",
    "url": "/project3/",
    "relUrl": "/project3/"
  },"9": {
    "doc": "üè† Home",
    "title": "Practice and Application of Data Science",
    "content": "DSC 80, Winter 2023 at UC San Diego . Tauhidur Rahman he/him . trahman@ucsd.edu . Lecture: MWF 12-12:50PM, Mandeville Auditorium B-210 . Jump to the current week . This site is under construction and everything is subject to change ‚ö†Ô∏è . Week 1 ‚Äì From BabyPandas to Pandas . Apr 3 LEC 1 Introduction . üé• ‚Ä¢ Ch. 1 . Apr 5 LEC 2 DataFrame Fundamentals . üé• ‚Ä¢ Ch. 2, Ed . Apr 7 LEC 3 More DataFrame Fundamentals . üé• ‚Ä¢ Ch. 2, 3 . Week 2 ‚Äì DataFrames . Apr 10 LEC 4 Grouping . üé• ‚Ä¢ Ch. 5.1 . Lab 1 Python, NumPy, and Pandas (due 4/10) . Apr 12 LEC 5 Pivoting and Simpson‚Äôs Paradox . üé• ‚Ä¢ Ch. 5.2 . SUR Welcome Survey . DIS 1 Lab 1 Reflection . üé• . Apr 13 PROJ 1 Gradebook üíØ (Checkpoint due 1/19) . Apr 14 LEC 6 Concatenating and Merging . üé• ‚Ä¢ Ch. 5.3-5.4 . Week 3 ‚Äì Messy Data . Apr 17 LEC 7 Relational Algebra, Data Cleaning . üé• ‚Ä¢ Ch. 4 . Lab 2 More Pandas (due 4/17) . Apr 19 LEC 8 Unfaithful Data, Hypothesis Testing . üé• ‚Ä¢ CIT 11 . DIS 2 Lab 2 Reflection . üé• . Apr 20 PROJ 1 Gradebook üíØ (due 4/20) . Apr 21 LEC 9 Hypothesis Testing . üé• ‚Ä¢ CIT 11 . Week 4 ‚Äì Statistical Testing and Missing Values . Apr 24 LEC 10 Permutation Testing . üé• ‚Ä¢ 5.5, CIT 12 . Lab 3 Grouping, Pivoting, and Combining (due 4/24) . Apr 26 LEC 11 Permutation Testing, Missingness Mechanisms . üé• ‚Ä¢ Ch. 6.1-6.2 . DIS 3 Lab 3 Reflection . üé• . Apr 27 PROJ 2 TBA . Apr 28 LEC 12 Identifying Missingness Mechanisms . üé• ‚Ä¢ Ch. 6.3-6.5 . Week 5 ‚Äì Imputation and HTTP . May 1 LEC 13 Imputation . üé• ‚Ä¢ Ch. 6.3-6.5 . Lab 4 Hypothesis and Permutation Testing (due 5/1) . May 3 LEC 14 HTTP Basics . üé• ‚Ä¢ Ch. 7.1-7.2 . DIS 4 Lab 4 Reflection . üé• . May 4 PROJ 2 TBA . May 5 Exam Midterm Exam (in-person during lecture) . Week 6 ‚Äì HTTP, HTML, and Regex . May 8 LEC 15 Requests and JSON . üé• ‚Ä¢ Ch. 7.1-7.2 . Lab 5 Missing Values and Imputation (due 5/8) . May 10 LEC 16 Web Scraping and Parsing HTML . üé• ‚Ä¢ Ch. 7.3 . DIS 5 Lab 5 Reflection . May 12 LEC 17 Regular Expressions . üé• ‚Ä¢ Ch. 8.1-8.2 . Week 7 ‚Äì Text Data . May 15 LEC 18 Text Features . üé• ‚Ä¢ Ch. 8.1-8.2 . Lab 6 HTTP and HTML (due 2/22) . May 17 LEC 19 Text Features, Continued . üé• ‚Ä¢ Ch. 8.1-8.2 . DIS 6 Lab 6 Reflection . üé• . May 18 PROJ 3 Exploratory Data Analysis üìä (due 5/18, no checkpoint) . May 19 LEC 20 Modeling and Regression . üé• ‚Ä¢ Ch. 8.1-8.2 . Week 8 ‚Äì Modeling and Feature Engineering . May 22 LEC 21 Feature Engineering . üé• ‚Ä¢ Ch. 8.2 . Lab 7 Regular Expressions and Text Data (due 5/22) . May 24 LEC 22 Feature Engineering, continued . üé• ‚Ä¢ Ch. 10.1 . DIS 7 Lab 7 Reflection . üé• . May 25 PROJ 4 Language Models üó£ (Checkpoint due 5/25) . May 26 LEC 23 Modeling in sklearn . üé• ‚Ä¢ Ch. 9.1, 11.1 . Week 9 ‚Äì sklearn Pipelines, Generalization, and Cross-Validation . May 29 No Class (Memorial Day observance) . May 31 LEC 24 sklearn Pipelines, Regression Evaluation . üé• ‚Ä¢ 9.2, 10.2, 11.2 . Lab 8 Modeling and Feature Engineering (due 5/29 at 4pm, no slipdays) . DIS 8 Lab 8 Reflection (due 3/11) . üé• . Jun 1 PROJ 4 Language Models üó£ (due 6/1) . Jun 2 LEC 25 Cross-Validation . üé• ‚Ä¢ Ch. 11.2 . Week 10 ‚Äì Classifier Evaluation, Fairness Criteria . Jun 5 LEC 26 Decision Trees, Grid Search, Multicollinearity . üé• . Lab 9 Pipelines (due 6/5) . Jun 7 LEC 27 Classifier Evaluation, Fairness Criteria . üé• . DIS 9 Lab 9 Reflection . üé• . Jun 9 LEC 28 Fairness Criteria, Conclusion . üé• . Week 11 ‚Äì Final Exam and Project 5 . Jun 14 EXAM Final Exam (11:30AM-2:30PM, in-person, location TBA) . Jun 15 PROJ 5 Model Building üõ† (due 6/15, no slip days) . ",
    "url": "/#practice-and-application-of-data-science",
    "relUrl": "/#practice-and-application-of-data-science"
  },"10": {
    "doc": "üè† Home",
    "title": "üè† Home",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"11": {
    "doc": "League of Legends Competitive Matches ‚å®Ô∏è",
    "title": "League of Legends Competitive Matches ‚å®Ô∏è",
    "content": " ",
    "url": "/project3/league-of-legends-competitive-matches/",
    "relUrl": "/project3/league-of-legends-competitive-matches/"
  },"12": {
    "doc": "League of Legends Competitive Matches ‚å®Ô∏è",
    "title": "Table of Contents",
    "content": ". | Getting the Data | Sample Questions | Cleaning and EDA | Assessment of Missingness | Hypothesis Testing | . Welcome to Summoner‚Äôs Rift! This dataset contains information of players and teams from over 10,000 League of Legends competitive matches. You‚Äôll probably want to be at least a little bit familiar with League of Legends and its terminology to use this dataset. If not, one of the other datasets may be more interesting to you. ",
    "url": "/project3/league-of-legends-competitive-matches/#table-of-contents",
    "relUrl": "/project3/league-of-legends-competitive-matches/#table-of-contents"
  },"13": {
    "doc": "League of Legends Competitive Matches ‚å®Ô∏è",
    "title": "Getting the Data",
    "content": "The data can be found on the website Oracle‚Äôs Elixir at the provided Google Drive link. To make sure that you‚Äôre able to satisfy the requirements of the project, use match data from 2022. You may use the older datasets if you wish, but keep in mind that League of Legends changes significantly between years; this can make it difficult to combine or make comparisons between datasets from different years. ",
    "url": "/project3/league-of-legends-competitive-matches/#getting-the-data",
    "relUrl": "/project3/league-of-legends-competitive-matches/#getting-the-data"
  },"14": {
    "doc": "League of Legends Competitive Matches ‚å®Ô∏è",
    "title": "Sample Questions",
    "content": ". | Looking at tier-one professional leagues, which league has the most ‚Äúaction-packed‚Äù games? Is the amount of ‚Äúaction‚Äù in this league significantly different than in other leagues? Note that you‚Äôll have to come up with a way of quantifying ‚Äúaction‚Äù. | Which competitive region has the highest win rate against teams outside their region? Note you will have to find and merge region data for this question as the dataset does not have it. | Which role ‚Äúcarries‚Äù (does the best) in their team more often: ADCs (Bot lanes) or Mid laners? | Is Talon (tutor Costin‚Äôs favorite champion) more likely to win or lose any given match? | . ",
    "url": "/project3/league-of-legends-competitive-matches/#sample-questions",
    "relUrl": "/project3/league-of-legends-competitive-matches/#sample-questions"
  },"15": {
    "doc": "League of Legends Competitive Matches ‚å®Ô∏è",
    "title": "Cleaning and EDA",
    "content": "Follow all of the steps in the Requirement: Cleaning and EDA section. Notes: . | Each 'gameid' corresponds to up to 12 rows ‚Äì one for each of the 5 players on both teams and 2 containing summary data for the two teams (try to find out what distinguishes those rows). After selecting your line of inquiry, make sure to remove either the player rows or the team rows so as not to have issues later in your analysis. | Many columns should be of type bool but are not. | . ",
    "url": "/project3/league-of-legends-competitive-matches/#cleaning-and-eda",
    "relUrl": "/project3/league-of-legends-competitive-matches/#cleaning-and-eda"
  },"16": {
    "doc": "League of Legends Competitive Matches ‚å®Ô∏è",
    "title": "Assessment of Missingness",
    "content": "Follow all of the steps in the Requirement: Assessment of Missingness section. ",
    "url": "/project3/league-of-legends-competitive-matches/#assessment-of-missingness",
    "relUrl": "/project3/league-of-legends-competitive-matches/#assessment-of-missingness"
  },"17": {
    "doc": "League of Legends Competitive Matches ‚å®Ô∏è",
    "title": "Hypothesis Testing",
    "content": "Follow all of the steps in the Requirement: Hypothesis Testing section. You can use the sample questions for inspiration. ",
    "url": "/project3/league-of-legends-competitive-matches/#hypothesis-testing",
    "relUrl": "/project3/league-of-legends-competitive-matches/#hypothesis-testing"
  },"18": {
    "doc": "Power Outages üîã",
    "title": "Power Outages üîã",
    "content": " ",
    "url": "/project3/power-outages/",
    "relUrl": "/project3/power-outages/"
  },"19": {
    "doc": "Power Outages üîã",
    "title": "Table of Contents",
    "content": ". | Getting the Data | Sample Questions | Cleaning and EDA | Assessment of Missingness | Hypothesis Test | . This dataset has major power outage data in the continental U.S. from January 2000 to July 2016. ",
    "url": "/project3/power-outages/#table-of-contents",
    "relUrl": "/project3/power-outages/#table-of-contents"
  },"20": {
    "doc": "Power Outages üîã",
    "title": "Getting the Data",
    "content": "The data is downloadable here. Note: If you are having a hard time with the ‚ÄúThis dataset‚Äù link, hold shift and click the link to open it into a new tab and then refresh that new tab. A data dictionary is available at this article under Table 1. Variable descriptions. ",
    "url": "/project3/power-outages/#getting-the-data",
    "relUrl": "/project3/power-outages/#getting-the-data"
  },"21": {
    "doc": "Power Outages üîã",
    "title": "Sample Questions",
    "content": ". | Where and when do major power outages tend to occur? | What are the characteristics of major power outages with higher severity? Variables to consider include location, time, climate, land-use characteristics, electricity consumption patterns, economic characteristics, etc. What risk factors may an energy company want to look into when predicting the location and severity of its next major power outage? | What characteristics are associated with each category of cause? | How have characteristics of major power outages changed over time? Is there a clear trend? | . ",
    "url": "/project3/power-outages/#sample-questions",
    "relUrl": "/project3/power-outages/#sample-questions"
  },"22": {
    "doc": "Power Outages üîã",
    "title": "Cleaning and EDA",
    "content": "Follow all of the steps in the Requirement: Cleaning and EDA section. Notes: . | The data is given as an Excel file rather than a CSV. Open the data in Google Sheets or another spreadsheet application and determine which rows and columns of the sheet should be ignored when loading the data in pandas. | Note that pandas can load multiple filetypes: pd.read_csv, pd.read_excel, pd.read_html, pd.read_json, etc. | . | The power outage start date and time is given by 'OUTAGE.START.DATE' and 'OUTAGE.START.TIME'. It would be preferable if these two columns were combined into one pd.Timestamp column. Combine 'OUTAGE.START.DATE' and 'OUTAGE.START.TIME' into a new pd.Timestamp column called 'OUTAGE.START'. Similarly, combine 'OUTAGE.RESTORATION.DATE' and 'OUTAGE.RESTORATION.TIME' into a new pd.Timestamp column called 'OUTAGE.RESTORATION'. | pd.to_datetime and pd.to_timedelta will be useful here. | . | . Tip: To visualize geospatial data, consider Folium or another geospatial plotting library. You can even embed Folium maps in your website! If fig is a folium.folium.Map object, then fig._repr_html_() evaluates to a string containing your plot as HTML; use open and write to write this string to an .html file. ",
    "url": "/project3/power-outages/#cleaning-and-eda",
    "relUrl": "/project3/power-outages/#cleaning-and-eda"
  },"23": {
    "doc": "Power Outages üîã",
    "title": "Assessment of Missingness",
    "content": "Follow all of the steps in the Requirement: Assessment of Missingness section. ",
    "url": "/project3/power-outages/#assessment-of-missingness",
    "relUrl": "/project3/power-outages/#assessment-of-missingness"
  },"24": {
    "doc": "Power Outages üîã",
    "title": "Hypothesis Test",
    "content": "Follow all of the steps in the Requirement: Hypothesis Testing section. You can use the sample questions for inspiration. ",
    "url": "/project3/power-outages/#hypothesis-test",
    "relUrl": "/project3/power-outages/#hypothesis-test"
  },"25": {
    "doc": "Project 5",
    "title": "Project 5 ‚Äì Model Building üõ†",
    "content": "Due Date: Thursday, March 23rd at 11:59PM (NO SLIP DAYS!) . ",
    "url": "/project5/#project-5--model-building-",
    "relUrl": "/project5/#project-5--model-building-"
  },"26": {
    "doc": "Project 5",
    "title": "Table of contents",
    "content": ". | Overview | Part 1: Analysis . | Requirement: Framing the Problem (15%) | Requirement: Baseline Model (35%) | Requirement: Final Model (35%) | Requirement: Fairness Analysis (15%) | Style | . | Part 2: Report | Example Prediction Problems . | Recipes and Ratings üçΩ | Power Outages üîã | League of Legends Competitive Matches ‚å®Ô∏è | . | Submission and Rubric . | Submission | Rubric | . | . ",
    "url": "/project5/#table-of-contents",
    "relUrl": "/project5/#table-of-contents"
  },"27": {
    "doc": "Project 5",
    "title": "Overview",
    "content": "Welcome to Project 5, the final assignment of the quarter! üëã . In Project 5, you will conduct an open-ended investigation into the dataset you chose for Project 3 (Recipes and Ratings, League of Legends, or Power Outages). Specifically, you will pose a prediction problem and train a model to solve it. Use the Example Prediction Problems section for inspiration. Project 5 is due on Thursday, March 23rd at 11:59PM. This is a hard deadline; you may NOT use slip days on this project. This is because we need to start grading projects right when you turn them in, so that there is enough time for you to make regrade requests before we submit grades to campus. Note that we will not be able to hold many office hours during Finals Week, so make sure to start early. Like Project 3, there is no checkpoint. Also, like in Project 3, you‚Äôll need to submit both a notebook and a website. Your Project 5 website must be separate from your Project 3 website; after the quarter is over, we suggest that you combine both into a single website that describes your complete exploration into your chosen dataset, from EDA to prediction. ",
    "url": "/project5/#overview",
    "relUrl": "/project5/#overview"
  },"28": {
    "doc": "Project 5",
    "title": "Part 1: Analysis",
    "content": "Before beginning your analysis, you‚Äôll need to set up a few things. | Pull the latest version of the dsc80-2023-wi repo. Within the projects/05-prediction folder, there is a template.ipynb notebook that you will use as a template for the project. If you delete the file or want another copy of the template, you can re-download it from here. This is where your analysis will live; you will submit this entire notebook to us. | Load the dataset you chose for Project 3 into your template notebook. | . Once you have your dataset loaded in your notebook, it‚Äôs time for you to find meaning in the real-world data you‚Äôve collected! Follow the steps below. Tip: For each step, we specify what must be done in your notebook and what must go on your website. We recommend you write everything in your notebook first, and then move things over to your website once you‚Äôve completed your analysis. Requirement: Framing the Problem (15%) . | Step | Analysis in Notebook | Report on Website | . | Problem Identification | Identify a prediction problem. Start by performing the same data cleaning you did in Project 3 (copying your code from Project 3 is fine, just include a comment about where you got the code from) and perform any further data cleaning you may need to solve your problem. | Clearly state your prediction problem and type (classification or regression). If you are building a classifier, make sure to state whether you are performing binary classification or multiclass classification. Report the response variable (i.e. the variable you are predicting) and why you chose it, the metric you are using to evaluate your model and why you chose it over other suitable metrics (e.g. accuracy vs. F1-score). Note: Make sure to justify what information you would know at the ‚Äútime of prediction‚Äù and to only train your model using those features. For instance, if we wanted to predict your final exam grade, we couldn‚Äôt use your Project 5 grade, because Project 5 is only due after the final exam! | . Requirement: Baseline Model (35%) . | Step | Analysis in Notebook | Report on Website | . | Baseline Model | Train a ‚Äúbaseline model‚Äù for your prediction task that uses at least two features. You can leave numerical features as-is, but you‚Äôll need to take care of categorical columns using an appropriate encoding. Implement all steps (feature transforms and model training) in a sklearn Pipeline. Note: Both now and in Final Model Step, make sure to evaluate your model‚Äôs ability to generalize to unseen data! There is no ‚Äúrequired‚Äù performance metric that your baseline model needs to achieve. | Describe your model and state the features in your model, including how many are quantitative, ordinal, and nominal, and how you performed any necessary encodings. Report the performance of your model and whether or not you believe your current model is ‚Äúgood‚Äù and why.Tip: Make sure to hit all of the points above: many projects in the past have lost points for not doing so. | . Requirement: Final Model (35%) . | Step | Analysis in Notebook | Report on Website | . | Final Model | Create a ‚Äúfinal‚Äù model that improves upon the ‚Äúbaseline‚Äù model you created in Step 2. Do so by engineering at least two new features from the data, on top of any categorical encodings you performed in Baseline Model Step. (For instance, you may use StandardScaler or QuantileTransformer transformers on quantitative columns.) Again, implement all steps in a sklearn Pipeline. While deciding what model and features to use, you must perform a search for the best model and hyperparameters (e.g. tree depth) to use amongst a list(s) of options, either by using GridSearchCV or through some manual iterative method. In your notebook, state which hyperparameters you plan to tune and why before actually tuning them.Note: You will not be graded on ‚Äúhow much‚Äù your model improved from Baseline Model Step to Final Model Step. What you will be graded on is on whether or not your model improved, as well as your thoughtfulness and effort in creating features, along with the other points above. | State the features you added and why they are good for the data and prediction task. Note that you can‚Äôt simply state ‚Äúthese features improved my accuracy‚Äù, since you‚Äôd need to choose these features and fit a model before noticing that ‚Äì instead, talk about why you believe these features improved your model‚Äôs performance from the perspective of the data generating process. Describe the model you chose, the hyperparameters that ended up performing the best, and the method you used to select hyperparameters and your overall model. Describe how your Final Model‚Äôs performance is an improvement over your Baseline Model‚Äôs performance.Optional: Include a visualization that describes your model‚Äôs performance, e.g. a confusion matrix, if applicable. | . Requirement: Fairness Analysis (15%) . | Step | Analysis in Notebook | Report on Website | . | Fairness Analysis | Perform a ‚Äúfairness analysis‚Äù of your Final Model from the previous step. That is, try and answer the question ‚Äúdoes my model perform worse for individuals in Group X than it does for individuals in Group Y?‚Äù, for an interesting choice of X and Y.As always, when comparing some quantitative attribute (in this case, something like precision or RMSE) across two groups, we use a permutation test. Let‚Äôs illustrate how this works with an example. Let‚Äôs suppose we have a sample voter dataset with columns 'Name', 'Age', and 'Voted', among others. We build a classifier that predicts whether someone voted (1) or didn‚Äôt (0).Here, we‚Äôll say our two groups are - ‚Äúyoung people‚Äù, people younger than 40- ‚Äúold people‚Äù, people older than 40Note that in this example, we manually created these groups by binarizing the 'Age' column in our dataset, and that‚Äôs fine. (Remember, the Binarizer transformer with a threshold of 40 can do this for us.)For our evaluation metric, we‚Äôll choose precision. (In Week 10‚Äôs lectures, we‚Äôll look at other evaluation metrics and related parity measures for classifiers; choose the one that is most appropriate to your prediction task. If you built a regression model, you cannot use classification metrics like precision or recall; instead, you must use RMSE or \\(R^2\\).)Now, we must perform a permutation test. Before doing so, we must clearly state a null and an alternative hypothesis.- Null Hypothesis: Our model is fair. Its precision for young people and old people are roughly the same, and any differences are due to random chance.- Alternative Hypothesis: Our model is unfair. Its precision for young people is lower than its precision for old people.From here, you should be able to implement the necessary permutation test. The only other guidance we will provide you with is that you should not be modifying your model to produce different results when computing test statistics; use only your final fitted model from Final Model Step. | Clearly state your choice of Group X and Group Y, your evaluation metric, your null and alternative hypotheses, your choice of test statistic and significance level, the resulting \\(p\\)-value, and your conclusion.Optional: Embed a visualization related to your permutation test in your website.Tip: When making writing your conclusions to the statistical tests in this project, never use language that implies an absolute conclusion; since we are performing statistical tests and not randomized controlled trials, we cannot prove that either hypothesis is 100% true or false.‚ÄúOnly a Sith deals in absolutes‚Äù - Obi-Wan Kenobi | . Style . While your website will neatly organized and tailored for public consumption, it is important to keep your analysis notebook organized as well. Follow these guidelines: . | Your work for each of the four project sections (Framing the Problem, Baseline Model, Final Model, and Fairness Analysis) described above should be completed in code cells underneath the Markdown header of that section‚Äôs name. | You should only include work that is relevant to posing, explaining, and answering the problem(s) you state in your website. You should include data quality, cleaning, though these should broadly be relevant to the question at hand. | Make sure to clearly explain what each component of your notebook means. Specifically: . | All plots should have titles, labels, and a legend (if applicable), even if they don‚Äôt make it into your website. Plots should be self-contained ‚Äì readers should be able to understand what they describe without having to read anything else. | All code cells should contain a comment describing how the code works (unless the code is self-explanatory ‚Äì use your best judgement). | . | . ",
    "url": "/project5/#part-1-analysis",
    "relUrl": "/project5/#part-1-analysis"
  },"29": {
    "doc": "Project 5",
    "title": "Part 2: Report",
    "content": "The purpose of your website is to provide the general public ‚Äì your classmates, friends, family, recruiters, and random internet strangers ‚Äì with an overview of your project and its findings, without forcing them to understand every last detail. We don‚Äôt expect the website creation process to take very much time, but it will certainly be rewarding. Once you‚Äôve completed your analysis and know what you will put in your website, start reading this section. Your website must clearly contain four headings: . | Framing the Problem | Baseline Model | Final Model | Fairness Analysis | . The specific content your website needs to contain is described in the ‚ÄúReport on Website‚Äù columns above. Make sure to also give your website a creative title that relates to the question you‚Äôre trying to answer, and clearly state your name(s). Your report will be in the form of a static website, hosted for free on GitHub Pages. More specifically, you‚Äôll use Jekyll, a framework built into GitHub Pages that allows you to create professional-looking websites just by writing Markdown (dsc80.com is built using Jekyll!). GitHub Pages does the ‚Äúhard‚Äù part of converting your Markdown to HTML. We won‚Äôt walk through the steps of creating Jekyll sites here because you already learned how to create them in Project 3. Speaking of Project 3 ‚Äì please add a link to your Project 3 website to the top of your website here, using something like: . Our exploratory data analysis on this dataset can be found here. ",
    "url": "/project5/#part-2-report",
    "relUrl": "/project5/#part-2-report"
  },"30": {
    "doc": "Project 5",
    "title": "Example Prediction Problems",
    "content": "Below, we provide example prediction problems for all three datasets. However, don‚Äôt restrict yourself to just these ‚Äì feel free to come up with your own! . Recipes and Ratings üçΩ . | Predict ratings of recipes. | Predict the number of minutes to prepare recipes. | Predict the number of steps in recipes. | Predict calories of recipes. | . Power Outages üîã . | Predict the severity (in terms of number of customers, duration, or demand loss) of a major power outage. | Predict the cause of a major power outage. | Predict the number and/or severity of major power outages in the year 2022. | Predict the electricity consumption of an area. | . League of Legends Competitive Matches ‚å®Ô∏è . | Predict if a team will win or lose a game. | Predict which role (top-lane, jungle, support, etc.) a player played given their post-game data. | Predict how long a game will take before it happens. | Predict which team will get the first Baron. | . ",
    "url": "/project5/#example-prediction-problems",
    "relUrl": "/project5/#example-prediction-problems"
  },"31": {
    "doc": "Project 5",
    "title": "Submission and Rubric",
    "content": "Submission . You will submit your project in two ways: . | By uploading a PDF version of your notebook to the specific ‚ÄúProject 5 Notebook PDF (Dataset)‚Äù assignment on Gradescope for your dataset. | To export your notebook as a PDF, first, restart your kernel and run all cells. Then, go to ‚ÄúFile &gt; Print Preview‚Äù. Then, save a print preview of the webpage as a PDF. There are other ways to save a notebook as a PDF but they may require that you have additional packages installed on your computer, so this is likely the most straightforward. | It‚Äôs fine if your plotly graphs don‚Äôt render properly in the PDF output of your notebook. However, make sure none of the code is cut off in your notebook‚Äôs PDF. If you need to, ‚Äústack‚Äù your code like in the example at the bottom of this page. You will lose 5% of the points available on this project if your code is cut off. | This notebook asks you to include a link to your website; make sure to do so. | . | By submitting a link to your website to the ‚ÄúProject 5 Website Link (All Datasets)‚Äù assignment on Gradescope. | . To both submissions, make sure to tag your partner. You don‚Äôt need to submit your actual .ipynb file anywhere. While your website must be public and you should share it with others, you should not make your code for this project available publicly. Since there are two assignments you need to submit to on Gradescope, we will treat your submission time as being the latter of your two submissions. So, if you submit to the ‚ÄúProject 5 Notebook PDF‚Äù assignment before the deadline but to the ‚ÄúProject 5 Website Link (All Datasets)‚Äù website one day late, overall, you will be charged a slip day. | There are a lot of moving parts to this assignment ‚Äì don‚Äôt wait until the last minute to try and submit! | A final reminder ‚Äì you cannot use slip days on Project 5! | . Rubric . Unlike in Project 3, we will not be providing you with the exact rubric that we will evaluate your project on. This is because an exact rubric would specify exactly what you need to do to build a model, but figuring out what to do is a large part of the project. (However, you can see how much each step is worth in the headings above.) . ",
    "url": "/project5/#submission-and-rubric",
    "relUrl": "/project5/#submission-and-rubric"
  },"32": {
    "doc": "Project 5",
    "title": "Project 5",
    "content": " ",
    "url": "/project5/",
    "relUrl": "/project5/"
  },"33": {
    "doc": "Recipes and Ratings üçΩÔ∏è",
    "title": "Recipes and Ratings üçΩÔ∏è",
    "content": " ",
    "url": "/project3/recipes-and-ratings/",
    "relUrl": "/project3/recipes-and-ratings/"
  },"34": {
    "doc": "Recipes and Ratings üçΩÔ∏è",
    "title": "Table of Contents",
    "content": ". | Getting the Data . | Recipes | Ratings | . | Sample Questions | Cleaning and EDA | Assessment of Missingness | Hypothesis Testing | . This dataset contains recipes and ratings from food.com. It was originally scraped and used by the authors of this recommender systems paper. ",
    "url": "/project3/recipes-and-ratings/#table-of-contents",
    "relUrl": "/project3/recipes-and-ratings/#table-of-contents"
  },"35": {
    "doc": "Recipes and Ratings üçΩÔ∏è",
    "title": "Getting the Data",
    "content": "Download the data here. You‚Äôll download two CSV files: . | RAW_recipes.csv contains recipes. | RAW_interactions.csv contains reviews and ratings submitted for the recipes in RAW_recipes.csv. | . We‚Äôve provided you with a subset of the raw data used in the original report, containing only the recipes and reviews posted since 2008, since the original data is quite large. A description of each column in both datasets is given below. Recipes . For context, you may want to look at an example recipe directly on food.com. | Column | Description | . | 'name' | Recipe name | . | 'id' | Recipe ID | . | 'minutes' | Minutes to prepare recipe | . | 'contributor_id' | User ID who submitted this recipe | . | 'submitted' | Date recipe was submitted | . | 'tags' | Food.com tags for recipe | . | 'nutrition' | Nutrition information in the form [calories (#), total fat (PDV), sugar (PDV), sodium (PDV), protein (PDV), saturated fat (PDV), carbohydrates (PDV)]; PDV stands for ‚Äúpercentage of daily value‚Äù | . | 'n_steps' | Number of steps in recipe | . | 'steps' | Text for recipe steps, in order | . | 'description' | User-provided description | . Ratings . | Column | Description | . | 'user_id' | User ID | . | 'recipe_id' | Recipe ID | . | 'date' | Date of interaction | . | 'rating' | Rating given | . | 'review' | Review text | . After downloading the datasets, you must follow the following steps to merge the two datasets and create a column containing the average rating per recipe: . | Left merge the recipes and interactions datasets together. | In the merged dataset, fill all ratings of 0 with np.nan. (Think about why this is a reasonable step, and include your justification in your website.) | Find the average rating per recipe, as a Series. | Add this Series containing the average rating per recipe back to the recipes dataset however you‚Äôd like (e.g., by merging). Use the resulting dataset for all of your analysis. (For the purposes of Project 3, the 'review' column in the interactions dataset doesn‚Äôt have much use.) | . ",
    "url": "/project3/recipes-and-ratings/#getting-the-data",
    "relUrl": "/project3/recipes-and-ratings/#getting-the-data"
  },"36": {
    "doc": "Recipes and Ratings üçΩÔ∏è",
    "title": "Sample Questions",
    "content": ". | What types of recipes tend to have the most calories? | What types of recipes tend to have higher average ratings? | What types of recipes tend to be healthier (i.e. more protein, fewer carbs)? | What is the relationship between the cooking time and average rating of recipes? | . There are a lot of other questions that can be asked from this data, so be creative! You are not limited to the sample questions above. ",
    "url": "/project3/recipes-and-ratings/#sample-questions",
    "relUrl": "/project3/recipes-and-ratings/#sample-questions"
  },"37": {
    "doc": "Recipes and Ratings üçΩÔ∏è",
    "title": "Cleaning and EDA",
    "content": "Follow all of the steps in the Requirement: Cleaning and EDA section. Tip: Some columns, like 'nutrition', contain values that look like lists, but are actually strings that look like lists. You may want to turn the strings into actual lists, or create columns for every unique value in those lists. For instance, per the data dictionary, each value in the 'nutrition' column contains information in the form \"[calories (#), total fat (PDV), sugar (PDV), sodium (PDV), protein (PDV), saturated fat (PDV), and carbohydrates (PDV)]\"; you could create individual columns in your dataset titled 'calories', 'total fat', etc. ",
    "url": "/project3/recipes-and-ratings/#cleaning-and-eda",
    "relUrl": "/project3/recipes-and-ratings/#cleaning-and-eda"
  },"38": {
    "doc": "Recipes and Ratings üçΩÔ∏è",
    "title": "Assessment of Missingness",
    "content": "Follow all of the steps in the Requirement: Assessment of Missingness section. Note: There are only three columns in the merged dataset that contain missing values; make sure you‚Äôre using the merged dataset for all of your analysis (and that you followed the steps at the top of this page exactly). ",
    "url": "/project3/recipes-and-ratings/#assessment-of-missingness",
    "relUrl": "/project3/recipes-and-ratings/#assessment-of-missingness"
  },"39": {
    "doc": "Recipes and Ratings üçΩÔ∏è",
    "title": "Hypothesis Testing",
    "content": "Follow all of the steps in the Requirement: Hypothesis Testing section. You can use the sample questions for inspiration. ",
    "url": "/project3/recipes-and-ratings/#hypothesis-testing",
    "relUrl": "/project3/recipes-and-ratings/#hypothesis-testing"
  },"40": {
    "doc": "üìö Resources",
    "title": "üìö Resources",
    "content": " ",
    "url": "/resources/",
    "relUrl": "/resources/"
  },"41": {
    "doc": "üìö Resources",
    "title": "Table of contents",
    "content": ". | Past Exams | Videos | Readings . | Textbooks | Extra Lecture Notebooks | Articles | Other Links | . | Regular Expressions | . ",
    "url": "/resources/#table-of-contents",
    "relUrl": "/resources/#table-of-contents"
  },"42": {
    "doc": "üìö Resources",
    "title": "Past Exams",
    "content": "Past exams with detailed solutions can be found at practice.dsc80.com. ",
    "url": "/resources/#past-exams",
    "relUrl": "/resources/#past-exams"
  },"43": {
    "doc": "üìö Resources",
    "title": "Videos",
    "content": ". | Working on Assignments in DSC 80 (Winter 2023 edition) | Working with the command-line in DSC 80 | . ",
    "url": "/resources/#videos",
    "relUrl": "/resources/#videos"
  },"44": {
    "doc": "üìö Resources",
    "title": "Readings",
    "content": "Textbooks . | notes.dsc80.com, our course notes. | Wes McKinney. ‚ÄúPython for Data Analysis‚Äù. | DSC 10 Course Notes ‚Äì great refresher on babypandas. | Principles and Techniques of Data Science, the textbook for Berkeley‚Äôs Data 100 course. | Computational and Inferential Thinking, the textbook for Berkeley‚Äôs Data 8 course. | . Extra Lecture Notebooks . | Lecture 11: Fast Permutation Tests. | Lecture 12: More Missingness Examples. | . Articles . | Views and Copies in pandas ‚Äì a great read if you‚Äôd like to learn more about the infamous SettingWithCopyWarning. | jwilber.me/permutationtest, a great visual explanation of permutation testing. | A Visual Introduction to Machine Learning and Model Tuning and the Bias-Variance Tradeoff, excellent visual descriptions of the last few weeks of the course (some terminology is different, but the ideas are the same). | . Other Links . | pandastutor.com, the equivalent of pythontutor.com for DataFrame manipulation. | . ",
    "url": "/resources/#readings",
    "relUrl": "/resources/#readings"
  },"45": {
    "doc": "üìö Resources",
    "title": "Regular Expressions",
    "content": ". | regex101.com, a helpful site to have open while writing regular expressions. | Python re library documentation and how-to. | regex ‚Äúcheat sheet‚Äù (taken from here). | . ",
    "url": "/resources/#regular-expressions",
    "relUrl": "/resources/#regular-expressions"
  },"46": {
    "doc": "üë©‚Äçüè´ Staff",
    "title": "üë©‚Äçüè´ Staff",
    "content": " ",
    "url": "/staff/",
    "relUrl": "/staff/"
  },"47": {
    "doc": "üë©‚Äçüè´ Staff",
    "title": "Instructor",
    "content": "Tauhidur Rahman he/him . trahman@ucsd.edu . Lecture: MWF 12-12:50PM, Mandeville Auditorium B-210 . Coming soon üëÄ . ",
    "url": "/staff/#instructor",
    "relUrl": "/staff/#instructor"
  },"48": {
    "doc": "üë©‚Äçüè´ Staff",
    "title": "Tutors",
    "content": "Yuxin Guo she/her . y5guo@ucsd.edu . Hi everyone! I‚Äôm Yuxin Guo, a third-year majoring in Data Science and Math-CS from China. This is my third time tutoring DSC 80, and I love DSC 80 because it covers various useful and interesting topics. Outside school, I enjoy swimming, surfing, listening to music, and playing with my catüê±. Feel free to reach out if you have any questions, I look forward to meeting you all! . Weiyue Li he/him . wel019@ucsd.edu . Hi, I‚Äôm Weiyue: a junior double majoring in Data Science and Applied Math. I‚Äôm originally from Shenzhen, China. DSC80 is one of the most useful courses in the lower-division that covers many techniques that you can use in your research and internships, and I‚Äôm thrilled to explore these topics with you! Fun fact: I am a Bears fanüêª‚¨áÔ∏è. Aishani Mohapatra she/her . aimohapatra@ucsd.edu . Coming soon üëÄ . Costin Smilovici he/him . csmilovi@ucsd.edu . Hey everyone, I‚Äôm Costin Smilovici a 4th year Data Science major. I‚Äôm from San Francisco. This is my 3rd time tutoring for DSC 80 and its has always been a joy help students with this course. Besides doing coding on the side, one hobby I have been really getting into right now is playing trading card games at local game stores. I‚Äôm looking forward to working with everybody this quarter so feel free to stop by my office hours for questions or to just talk! . Yujia Wang she/her . yuw103@ucsd.edu . Hi everyone! My name is Yujia(Joy), and I‚Äôm a third-year student double majoring in Data Science and Math&amp;Econ. I was born and raised in China for the first fourteen years of my life, then my family moved to San Diego. While this is my third time tutoring, it‚Äôs my first time tutoring DSC80. I am really excited to tutor for this class, and I am willing to provide as much help as possible to everyone in this class. I look forward to meeting you all, and please feel free to reach out if you have any questions! . Tiffany Yu she/her . z5yu@ucsd.edu . Hi! My name is Tiffany and I am a third-year Data Science student from Warren College. Besides studying, I love dancing and playing the piano. I have tutored DSC 10 three quarters and this is my second time tutoring DSC 80. I am super excited to pass on my passion to all of you. Feel free to chat with me about anything related to data science! :) . Diego Zavalza he/him . dzavalza@ucsd.edu . Coming soon üëÄ . ",
    "url": "/staff/#tutors",
    "relUrl": "/staff/#tutors"
  },"49": {
    "doc": "üìñ Syllabus",
    "title": "üìñ Syllabus",
    "content": " ",
    "url": "/syllabus/",
    "relUrl": "/syllabus/"
  },"50": {
    "doc": "üìñ Syllabus",
    "title": "Table of contents",
    "content": ". Coming soon üëÄ . ",
    "url": "/syllabus/#table-of-contents",
    "relUrl": "/syllabus/#table-of-contents"
  },"51": {
    "doc": "üôã‚Äç‚ôÇÔ∏è Tech Support",
    "title": "üôã‚Äç‚ôÇÔ∏è Tech Support",
    "content": " ",
    "url": "/tech_support/",
    "relUrl": "/tech_support/"
  },"52": {
    "doc": "üôã‚Äç‚ôÇÔ∏è Tech Support",
    "title": "Table of contents",
    "content": ". | Introduction | Working Locally (Recommended) . | Installing Python | Replicating the Gradescope Environment | Git | Choosing a Text Editor or IDE | . | Working Remotely via DataHub . | ‚ö†Ô∏è Warning! | Installing or Updating Python Packages | JupyterLab | Git | Troubleshooting DataHub | . | . ",
    "url": "/tech_support/#table-of-contents",
    "relUrl": "/tech_support/#table-of-contents"
  },"53": {
    "doc": "üôã‚Äç‚ôÇÔ∏è Tech Support",
    "title": "Introduction",
    "content": "Assignments in DSC 80 are mostly coding assignments, so it‚Äôs important to make sure that your computing environment is set up properly. There are two ways to go about things: you can set up a local environment or use a remote environment that is largely pre-configured. On this page, we‚Äôll talk about both options. Writing code locally, on your personal computer, is our preferred option. We won‚Äôt lie ‚Äì it involves a little more time to set up and a steeper learning curve. But in the long run, you‚Äôll likely find the local environment more comfortable and faster since you can customize it to your own needs. Additionally, setting up your own local Python environment is something you‚Äôll be expected to do when working as a data scientist, so it‚Äôs a good idea to start now. There has been a lot written about how to set up a Python environment, so we won‚Äôt reinvent the wheel. This page will only be a summary; Google will be your main resource. But always feel free to come to a staff member‚Äôs office hours if you have a question about setting up your environment, using Git, or similar ‚Äî we‚Äôre here to help. This video contains a walkthrough of many of the details on this page. It‚Äôs not a substitute for reading this page, though. ",
    "url": "/tech_support/#introduction",
    "relUrl": "/tech_support/#introduction"
  },"54": {
    "doc": "üôã‚Äç‚ôÇÔ∏è Tech Support",
    "title": "Working Locally (Recommended)",
    "content": "Working locally simply refers to developing code using software installed on your own machine. For this class, the software you'll need includes Python 3.8, a few specific Python packages, Git, and a text editor. Installing Python . There are several ways of installing Python on your own computer. We recommend downloading the Anaconda Python distribution and following the instructions for installing it. You should install the standard Anaconda (not Miniconda) with Python 3.8. As mentioned above, Anaconda is a Python distribution. This means it comes with many useful Python packages, including, for instance, pandas. If you should need to install a new Python package, you can use the conda command. You'll need to open a terminal. On Windows, you can use the Anaconda Prompt; on macOS or Linux, you can use the terminal app that comes with the operating system, or install one (Alacritty is a popular choice). Then, inside the terminal, type conda install &lt;package_name&gt;, where &lt;package_name&gt; is replaced by the name of the package you want to install, and hit enter. Anaconda comes with pandas, numpy, and many other data science packages. You will, however, need to install otter-grader; this is the autograder package that runs the tests in labs, projects, etc. You can do so by running: pip install otter-grader in a terminal. Replicating the Gradescope Environment . If you followed these steps before 7PM on 1/10, and you‚Äôre unable to run Jupyter Notebooks, you may want to uninstall your conda environment and reinstall it with the following updated requirements.txt file. To delete your dsc80 conda environment, use conda env remove -n dsc80. Gradescope has a package environment which it uses to autograde your work. It is advised to create the same environment so that there are no issues due to version changes during development vs. evaluation. Please follow the below steps to create the environment with required packages. | 1. Create a requirements.txt file with the following text: | . matplotlib==3.4.3 numpy==1.21.2 otter-grader==3.1.4 notebook==6.4.12 pandas==1.3.3 Pillow==8.3.2 pydantic==1.8.2 PyYAML==5.4.1 requests==2.26.0 tqdm==4.62.3 urllib3==1.26.7 scikit-learn==1.0 seaborn==0.11.2 beautifulsoup4==4.10.0 . | 2. In Terminal, create a new conda environment: conda create -n dsc80 python=3.8 . | 3. Activate the environment: conda activate dsc80 . | 4. Install the requirements in the new env: pip install -r requirements.txt . | . Every time you work on DSC 80, activate this environment by running conda activate dsc80 in your terminal. To open a Jupyter Notebook, use the jupyter notebook command in your terminal. Git . All of our course materials, including your assignments, are hosted on GitHub in this Git repository. This means that you‚Äôll need to download and use Git in order to work with the course materials. Git is a version control system. In short, it is used to keep track of the history of a project. With Git, you can go back in time to any previous version of your project, or even work on two different versions (or \"branches\") in parallel and \"merge\" them together at some point in the future. We'll stick to using the basic features of Git in DSC 80. There are Git GUIs, and you can use them for this class. You can also use the command-line version of Git. To get started, you'll need to \"clone\" the course repository. The command to do this is: . git clone https://github.com/dsc-courses/dsc80-2023-wi . This will copy the repository to a directory on your computer. To bring in the latest version of the repository, run git pull in your local repository. This will not overwrite your work. In fact, Git is designed to make it very difficult to lose work (although it's still possible!). Choosing a Text Editor or IDE . In this class, you will need to use a combination of editors for doing your assignments: The python files should be developed with an IDE (for syntax highlighting and running doctests) and the data/results should be analyzed/presented in Jupyter Notebooks. Below is an incomplete list of IDEs you might want to try. For more information about them, feel free to ask the course staff. If you‚Äôre curious, Suraj uses VSCode to edit .py files and the vanilla Jupyter environment to edit notebooks. | The JupyterLab text editor: see below. Can be used to edit both notebooks and .py files. | VSCode: Microsoft Visual Studio Code. Currently very popular, and can also be used to edit both notebooks and .py files. | sublime: A favorite text editor of hackers, famous for its multiple cursors. A good, general-purpose choice. | atom: GitHub‚Äôs editor. Pretty nice fully featured IDE. Can only work locally. | PyCharm (IntelliJ): Those who feel at home coding Java. Can only work locally. | nano: available on most unix commandlines (e.g. DataHub Terminal). If you use this for more than changing a word or two, you'll hate your life. | (neo)vim: lightweight, productive text-editor that might be the most efficient way to edit text, if you can ever learn how to use it. Beware opening vim, as you may never figure out how to quit (literally). Justin Eldridge‚Äôs text editor of choice. | emacs: A text editor for those who prefer a life of endless toil. Endlessly customizable, it promises everything, but you‚Äôre never good enough to deliver. Its keyboard shortcuts are guaranteed to give you carpal tunnel. Aaron Fraenkel‚Äôs text editor of choice. | . ",
    "url": "/tech_support/#working-locally-recommended",
    "relUrl": "/tech_support/#working-locally-recommended"
  },"55": {
    "doc": "üôã‚Äç‚ôÇÔ∏è Tech Support",
    "title": "Working Remotely via DataHub",
    "content": "Working remotely means using an environment that someone else set up for you on a computer far, far away, usually through the browser. This is the way you wrote code in DSC 10, for instance. There's nothing wrong with this, per se, and it is simpler, but you should think of this option as developing with \"training wheels\". Eventually, you will need to learn how to set up your own Python environment, and now is as good a time as any. There are servers available to use at datahub.ucsd.edu. These are a lot like the DataHub servers that you used in DSC 10, however they are customized for this course. After logging in with your UCSD account, you will be taken the familiar juptyer landing page. The server you are logged into has ~4GB of RAM available, and has Python with all the necessary packages. ‚ö†Ô∏è Warning! . DataHub outages are not uncommon, and they can be expected to occur once or twice per quarter (sometimes more). Outages typically last for a few hours or less, but they can prevent you from working on your assignment. Since we do not manage DataHub, we cannot make any guarantees about its availability. DataHub crashes that prevent you from turning in or working on your assignment near the deadline are typically handled via the usual slip day mechanism. If DataHub has been down for a long time (more than 24 hours), let us know and we'll consider a blanket extension ‚Äì though this has very rarely (never?) happened. Our advice is to use a local development environment, or to at least have one as available as a backup option. If you decide to use DataHub as your first choice, you should keep an extra slip day or two in reserve in case the server crashes. Installing or Updating Python Packages . To update a package (e.g. pandas) on DataHub, you'll need to use the command line. To do this, open ‚ÄúNew &gt; Terminal‚Äù and type: . pip install --user --upgrade pandas . followed by the enter key to run the command. One package that you'll likely need to install is otter-grader. This package provides the autograder that checks your answers in the labs and projects. JupyterLab . The remote servers have a development environment installed on them, however, it‚Äôs non-intuitive how to access it. Once on the landing page, the url should read something like: . https://datahub.ucsd.edu/user/USER/tree . You can access the IDE (integrate development environment) by changing \"tree\" to \"lab\". This brings up JupyterLab. The url should look something like this: . https://datahub.ucsd.edu/user/USER/lab . For more information on this IDE, you can see read about it here. From within JupyterLab, you can: . | Use a Python console | Run Jupyter notebooks | Use a terminal (e.g. to pull git repos) | Develop Python code in .py files | . Git . Whether you work locally or use DataHub, you‚Äôll need to pull assignments from GitHub. If you work on DataHub, you‚Äôll have to pull from GitHub using the command-line. To do this, open ‚ÄúNew &gt; Terminal‚Äù and, to get the course repository for the first time, type: . git clone https://github.com/dsc-courses/dsc80-2023-wi . Then, open up the file-tree in the original Jupyter tab, and you should see all the course files now there. If you have already cloned the repository, and just want to get the latest files, type git pull and you should see the updated files. Troubleshooting DataHub . What if I accidentally clicked a different class instead of DSC 80 when logging into DataHub, or what if my DataHub doesn‚Äôt load? . | If you are already logged into DataHub, click ‚ÄúControl Panel‚Äù in the top right. (If your DataHub never launched in the first place, proceed to the next step.) . | In the toolbar at appears at datahub.ucsd.edu, click ‚ÄúServices‚Äù then click ‚Äúmanual-resetter‚Äù, then click ‚ÄúReset‚Äù. If a pop-up box appears, that‚Äôs okay. | Log back into DataHub again and it should allow you to select a course ‚Äì select DSC 80. | . ",
    "url": "/tech_support/#working-remotely-via-datahub",
    "relUrl": "/tech_support/#working-remotely-via-datahub"
  }
}
