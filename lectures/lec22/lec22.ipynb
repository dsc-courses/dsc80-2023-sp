{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7904f660",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "TEMPLATE = 'seaborn'\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca51859",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 22 – Pipelines, Generalization\n",
    "\n",
    "## DSC 80, Spring 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22824200",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- The modeling process.\n",
    "- Transformers in `sklearn`.\n",
    "- Pipelines.\n",
    "- Generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a95ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The modeling process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d14ddab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The modeling process\n",
    "\n",
    "<center><img src=\"imgs/image_0.png\" width=\"60%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26e1d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Create (engineer) features to best reflect the \"meaning\" behind data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea2cf7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Choose a model that is appropriate to capture the relationships between features ($X$) and the target/response ($y$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058f9b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Select a loss function and fit the model (i.e., determine $w^*$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cbf67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Evaluate the model (e.g. using RMSE or $R^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a261e8a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**We can perform all of the above directly in `sklearn`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7f04d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `preprocessing` and `linear_model`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc00479",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the **feature engineering** step of the modeling pipeline, we will use `sklearn`'s [`preprocessing`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module.\n",
    "\n",
    "<center><img src=\"imgs/feature_part.png\" width=\"30%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795bf90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the **model creation** step of the modeling pipeline, we will use `sklearn`'s [`linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) module, as we've already seen. `linear_model.LinearRegression` is an example of an **estimator** class.\n",
    "\n",
    "<center><img src=\"imgs/model_part.png\" width=\"36%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813b18b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41403a4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformer classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18144d7c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Transformers** take in \"raw\" data and output \"processed\" data. They are used for **creating features**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d6dba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The input to a transformer should be a multi-dimensional `numpy` array.\n",
    "    - Inputs can be DataFrames, but `sklearn` only looks at the values (i.e. it calls `to_numpy()` on input DataFrames)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c095ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The output of a transformer is a `numpy` array (never a DataFrame or Series)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10160f90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Transformers, like most relevant features of `sklearn`, are **classes**, not functions, meaning you need to instantiate them and call their methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f64cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case study: Restaurant tips 🧑‍🍳\n",
    "\n",
    "We'll continue working with our trusty `tips` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = px.data.tips()\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159c00f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example transformer: `Binarizer`\n",
    "\n",
    "The `Binarizer` transformer allows us to map a quantitative sequence to a sequence of 1s and 0s, depending on whether values are above or below a threshold.\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize with parameters| `binar = Binarizer(thresh)` | set x=1 if x > thresh, else 0|\n",
    "|Transform data in a dataset | `feat = binar.transform(data)` | Binarize all columns in `data`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec5acd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, we need to import the relevant class from `sklearn.preprocessing`. (Tip: import just the relevant classes you need from `sklearn`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95076bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80843c5a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's try binarizing `'total_bill'`. We'll say a \"large\" bill is one that is **strictly** greater than \\$20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f125d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips['total_bill'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8186086",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, we initialize a `Binarizer` object with the threshold we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = Binarizer(threshold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611bda9c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, we call `bi`'s `transform` method and pass it the data we'd like to transform. Note that its input and output are both 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e13219",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_bills = bi.transform(tips[['total_bill']]) # Must give transform a 2D array/DataFrame.\n",
    "transformed_bills[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc577d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example transformer: `StdScaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666bf33a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `StdScaler` **standardizes** data using the mean and standard deviation of the data.\n",
    "\n",
    "$$z(x_i) = \\frac{x_i - \\text{mean of } x}{\\text{SD of } x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aef75b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unlike `Binarizer`, `StdScaler` **requires some knowledge (mean and SD) of the dataset before transforming**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355fa270",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, we need to **`fit`** an `StdScaler` transformer before we can use the `transform` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae155940",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Typical usage: fit transformer on a sample, use that fit transformer to transform future data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bbeaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example transformer: `StdScaler`\n",
    "\n",
    "It only makes sense to standardize the already-quantitative features of `tips`, so let's select just those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_quant = tips[['total_bill', 'size']]\n",
    "tips_quant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d684be1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's initialize a `StandardScaler` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03790dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the following **does not work!** The error message is very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b96342",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.transform(tips_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6d884",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead, we need to first call the `fit` method on `stdscaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is like saying \"determine the mean and SD of each column in tips_quant\".\n",
    "stdscaler.fit(tips_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f4d3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, `transform` will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d812b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column is 'total_bill', second column is 'size'.\n",
    "tips_quant_z = stdscaler.transform(tips_quant)\n",
    "tips_quant_z[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fc339",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can also access the mean and variance `stdscaler` computed for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f412d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89daaa5f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that we can call `transform` on DataFrames other than `tips_quant`. We will do this often – fit a transformer on one dataset (training data) and use it to transform other datasets (test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.transform(tips_quant.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6481695",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `StdScaler` summary\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize with parameters| `stdscaler = StandardScaler()` | z-score the data (no parameters) |\n",
    "|Fit the transformer| `stdscaler.fit(X)` | Compute the mean and SD of `X`|\n",
    "|Transform data in a dataset | `feat = stdscaler.transform(X_new)` | z-score `X_new` with mean and SD of `X`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d017d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example transformer: `OneHotEncoder`\n",
    "\n",
    "Let's keep just the categorical columns in `tips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f3536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_cat = tips[['sex', 'smoker', 'day', 'time']]\n",
    "tips_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af220ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Like `StdScaler`, we will need to `fit` our `OneHotEncoder` transformer before it can transform anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f412324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(tips_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a93b50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can look at the unique values (i.e. categories) in each column by using the `categories_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d85564",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.transform(tips_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ae7a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since the resulting matrix is **sparse** – most of its elements are 0 – `sklearn` uses a more efficient representation than a regular `numpy` array. That's no issue, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aabf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.transform(tips_cat).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6195a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that the column names from `tips_cat` are no longer stored anywhere (remember, `fit` converts the input to a `numpy` array before proceeding).\n",
    "\n",
    "We can use the `get_feature_names` method on `ohe` to access the names of the one-hot-encoded columns, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names() # x0, x1, x2, and x3 correspond to column names in tips_cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ohe.transform(tips_cat).toarray(), \n",
    "             columns=ohe.get_feature_names()) # If we need a DataFrame back, for some reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833fac9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f71de5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/image_0.png\" width=\"50%\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "So far, we've used transformers for feature engineering and models for prediction. We can combine these steps into a single `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1ef16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Pipeline`s in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d88ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To instantiate a `Pipeline`, we must provide a list with zero or more transformers followed by a single model.\n",
    "    - All \"steps\" must have `fit` methods, and all but the last must have `transform` methods.\n",
    "    - Template: `pl = Pipeline([feat_trans1, feat_trans2, ..., mdl])`.\n",
    "    \n",
    "```py\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9eac2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once a `Pipeline` is instantiated, you can fit **all** steps (transformers and model) using a single call to the `fit` method.\n",
    "```py\n",
    "pl.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708e3c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To make predictions using **raw, untransformed data**, use `pl.predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af3265",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The actual list we provide `Pipeline` with must be a list of **tuples**, where\n",
    "    - The first element is a \"name\" (that we choose) for the step.\n",
    "    - The second element is a transformer or estimator instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86669e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our first `Pipeline`\n",
    "\n",
    "Let's build a `Pipeline` that:\n",
    "- One hot encodes the categorical features in `tips`.\n",
    "- Fits a regression model on the one hot encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03859c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_cat = tips[['sex', 'smoker', 'day', 'time']]\n",
    "tips_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cf9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49f763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "    ('one-hot', OneHotEncoder()),\n",
    "    ('lin-reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ef92c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that `pl` is instantiated, we `fit` it the same way we would fit the individual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03991a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(tips_cat, tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b902c9f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, to make predictions using **raw data**, all we need to do is use `pl.predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict([['Female', 'Yes', 'Sat', 'Lunch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(tips_cat.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8518b9",
   "metadata": {},
   "source": [
    "`pl` performs **both** feature transformation and prediction with just a single call to `predict`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea45098",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can access individual \"steps\" of a `Pipeline` through the `named_steps` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c6298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.named_steps['one-hot'].transform(tips_cat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f17401",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps['lin-reg'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c7b1ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`pl` also has a `score` method, the same way a fit `LinearRegression` instance does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.score(tips_cat, tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fc2ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More sophisticated `Pipeline`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2afec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the previous example, we one hot encoded every input column. **What if we want to perform different transformations on different columns?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c421ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Solution:** Use a `ColumnTransformer`.\n",
    "    - Instantiate a `ColumnTransformer` using a list of tuples, where:\n",
    "        - The first element is a \"name\" we choose for the transformer.\n",
    "        - The second element is a transformer instance (e.g. `OneHotEncoder()`).\n",
    "        - The third element is a **list of relevant column names**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1aa826",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `ColumnTransformer` is extremely useful, but it was only added to `sklearn` in 2018!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477c359",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Planning our first `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd28cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ecc2b",
   "metadata": {},
   "source": [
    "Let's perform different transformations on the quantitative and categorical features of `tips` (note that we are not transforming `'tip'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_features = tips.drop('tip', axis=1)\n",
    "tips_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b564152",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We will leave the `'total_bill'` column untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7505b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To the `'size'` column, we will apply the `Binarizer` transformer with a threshold of 2 (big tables vs. small tables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b247986",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To the categorical columns, we will apply the `OneHotEncoder` transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4878d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In essence, we will create a transformer that reproduces the following DataFrame:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>size</th>\n",
    "      <th>x0_Female</th>\n",
    "      <th>x0_Male</th>\n",
    "      <th>x1_No</th>\n",
    "      <th>x1_Yes</th>\n",
    "      <th>x2_Fri</th>\n",
    "      <th>x2_Sat</th>\n",
    "      <th>x2_Sun</th>\n",
    "      <th>x2_Thur</th>\n",
    "      <th>x3_Dinner</th>\n",
    "      <th>x3_Lunch</th>\n",
    "      <th>total_bill</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>16.99</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>1</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>10.34</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>1</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>21.01</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>23.68</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>1</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>24.59</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89393533",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building a `Pipeline` using a `ColumnTransformer`\n",
    "\n",
    "Let's start by creating our `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e5854",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('size', Binarizer(threshold=2), ['size']),\n",
    "        ('categorical_cols', OneHotEncoder(), ['sex', 'smoker', 'day', 'time'])\n",
    "    ],\n",
    "    remainder='passthrough' # Specify what to do with all other columns ('total_bill' here) – drop or passthrough.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f5569",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, let's create a `Pipeline` using `preproc` as a transformer, and `fit` it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "    ('preprocessor', preproc), \n",
    "    ('lin-reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(tips_features, tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5db465",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Prediction is as easy as calling `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3f595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we fit the Pipeline using tips_features, not tips_features.head()!\n",
    "pl.predict(tips_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a570033",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can even call each transformer in `pl['preprocessor']` individually to re-create the transformed DataFrame. (There's no _practical_ reason to do this, it's more for illustration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for trans in pl['preprocessor'].transformers_:\n",
    "    if isinstance(trans[1], str) and trans[1] == 'passthrough':\n",
    "        df = tips_features.iloc[:, trans[2]]\n",
    "    else:\n",
    "        vals = trans[1].transform(tips_features[trans[2]])\n",
    "        columns = trans[2]\n",
    "        if str(trans[1]) == 'OneHotEncoder()':\n",
    "            vals = vals.toarray()\n",
    "            columns = trans[1].get_feature_names()\n",
    "        df = pd.DataFrame(vals, columns=columns)\n",
    "    dfs.append(df)\n",
    "\n",
    "pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b1a71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: `FunctionTransformer`\n",
    "\n",
    "A transformer you'll often use as part of a `ColumnTransformer` is the `FunctionTransformer`, which enables you to use your own functions on entire columns. Think of it as the `sklearn` equivalent of `apply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5dbb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FunctionTransformer(np.sqrt)\n",
    "f.transform([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f679d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: `Pipeline`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac91a5f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `Pipeline`s are powerful because they allow you to perform feature engineering and training/prediction all through a single object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa731c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It's important to understand what each step of a `Pipeline` does. Neural networks work _similarly_ to `sklearn` `Pipeline`s, in that they follow a well-defined sequence of steps to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f29f107",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b0dd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88707c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You and Billy are studying for an upcoming exam. You both decide to test your understanding by taking a **practice exam**.\n",
    "    - Your logic: If you do well on the practice exam, you should do well on the real exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befcf411",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You each take the practice exam once and look at the solutions afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58b934",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Your strategy**: Memorize the answers to all practice exam questions, e.g. \"Question 1: A; Question 2: C; Question 3: A.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a912c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Billy's strategy**: Learn high-level concepts from the solutions, e.g. \"data are NMAR if the likelihood of missingness depends on the missing values themselves.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb8107",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Who will do better on the **practice exam**? Who will probably do better on the **real exam**? 🧐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32bb5b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating the quality of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4def24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far, we've computed the RMSE (and $R^2$) of our fit regression models on the **data that we used to fit them**, i.e. the **training data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98576c7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We've said that Model A is **better** than Model B if Model A's RMSE is **lower** than Model B's RMSE.\n",
    "    - Remember, our **training data** is a sample from the data generating process.\n",
    "    - Just because a model fits the training data well doesn't mean it will **generalize** and work well on **similar, unseen samples**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ddd1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Overfitting and underfitting\n",
    "\n",
    "Let's collect two samples $\\{(x_i, y_i)\\}$ from the same **data generating process**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39134087",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23) # For reproducibility.\n",
    "\n",
    "def sample_dgp(n=100):\n",
    "    x = np.linspace(-2, 3, n)\n",
    "    y = x ** 3 + (np.random.normal(0, 3, size=n))\n",
    "    return pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "sample_1 = sample_dgp()\n",
    "sample_2 = sample_dgp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add47e11",
   "metadata": {},
   "source": [
    "For now, let's just look at Sample 1. The relationship between $x$ and $y$ is roughly **cubic**; that is, $y \\approx x^3$ (remember, in reality, you won't get to see the DGP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8dc21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.scatter(sample_1, x='x', y='y', title='Sample 1', template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10ff0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial regression\n",
    "\n",
    "Let's fit three **polynomial** models on Sample 1:\n",
    "- Degree 1.\n",
    "- Degree 3.\n",
    "- Degree 25.\n",
    "\n",
    "The `PolynomialFeatures` transformer will be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20eadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform fits and transforms the same input.\n",
    "d2 = PolynomialFeatures(3)\n",
    "d2.fit_transform(np.array([1, 2, 3, 4, -2]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771f73d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below, we look at our three models' predictions on Sample 1 (which they were trained on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f8a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the definition of train_and_plot in util.py if you're curious as to how the plotting works.\n",
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_1, degs=[1, 3, 25])\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a98fb8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The degree 25 polynomial has the lowest RMSE on Sample 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242734c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do the same fit polynomials look on Sample 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f83da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_2, degs=[1, 3, 25])\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527e712",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The degree 3 polynomial has the lowest RMSE on Sample 2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df906f64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that **we didn't get to see Sample 2 when fitting our models**! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81678178",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, it seems that the degree 3 polynomial **generalizes better** to unseen data than the degree 25 polynomial does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cabe0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we fit a degree 1, degree 3, and degree 25 polynomial **on Sample 2** as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb427170",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_multiple_models(sample_1, sample_2, degs=[1, 3, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5bd0f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key idea**: Degree 25 polynomials seem to **vary more when trained on different samples** than degree 3 and 1 polynomials do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f28d03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias and variance\n",
    "\n",
    "The training data we have access to is a sample from the DGP. We are concerned with our model's ability to **generalize** and work well on **different datasets** drawn from the same DGP.\n",
    "\n",
    "Suppose we **fit** a model $H$ (e.g. a degree 3 polynomial) on **several different datasets** from a DGP. There are three sources of error that arise:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad630139",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ⭐️ **Bias**: **The expected deviation between a predicted value and an actual value**.\n",
    "    - In other words, **for a given $x_i$, how far is $H(x_i)$ from the true $y_i$, on average?**\n",
    "    - Low bias is good! ✅\n",
    "    - High bias is a sign of **underfitting**, i.e. that our model is too **basic** to capture the relationship between our features and response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064ccd0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ⭐️ **Model variance (\"variance\")**: **The variance of a model's predictions**.\n",
    "    - In other words, **for a given $x_i$, what is the variance of $H(x_i)$ across all datasets**?\n",
    "    - Low model variance is good! ✅\n",
    "    - High model variance is a sign of **overfitting**, i.e. that our model is too **complicated** and is prone to fitting to the noise in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50296cc6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Observation variance**: The variance due to the random noise in the process we are trying to model (e.g. measurement error). _We can't control this, without collecting more data!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba48089",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, suppose:\n",
    "- The <span style='color:#c6283f'><b>red bulls-eye</b></span> represents your **true weight and height** 🧍.\n",
    "- The <span style='color:#080c6f'><b>dark blue darts</b></span> represent **predictions of your weight and height** using different models that were fit on the same DGP. \n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/image_5.png\" width=\"40%\"></center>\n",
    "\n",
    "We'd like our models to be in the top left, but in practice that's hard to achieve!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101d553",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e65dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Pipelines in `sklearn` combine one or more transformers with a single model (estimator), allowing us to perform feature engineering and prediction through a single object.\n",
    "- We want to build models that **generalize** well to unseen data.\n",
    "    - Models that have **high bias** are too simple to represent complex relationships in data, and underfit.\n",
    "    - Models that have **high variance** are overly complex for the relationships in the data, and vary a lot when fit on different datasets. Such models overfit to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6e3a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next time\n",
    "\n",
    "How do we choose the right model complexity, so that our model has the right \"balance\" between bias and variance?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
