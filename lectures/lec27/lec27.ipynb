{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bd6be",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "TEMPLATE = 'seaborn'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e317133",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 27 â€“ Fairness, Conclusion\n",
    "\n",
    "## DSC 80, Spring  2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea175d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Fairness.\n",
    "- Parity measures.\n",
    "- Example: Loan approval.\n",
    "- Parting thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e62e47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75d5ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fairness: why do we care?\n",
    "\n",
    "- Sometimes, a model performs better for certain groups than others; in such cases we say the model is **unfair**.\n",
    "- Since ML models are now used in processes that significantly affect human lives, it is important that they are fair!\n",
    "    * Job applications and college admissions.\n",
    "    * Criminal sentencing and parole grants.\n",
    "    * Predictive policing.\n",
    "    * Credit and loans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97633450",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: COMPAS and recidivism prediction\n",
    "\n",
    "COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a \"black-box\" model that estimates the likelihood that someone who has commited a crime will recidivate (commit another crime).\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/compas.jpeg\"></center>\n",
    "\n",
    "[Propublica found](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) that the model's false positive rate is higher for African-Americans than it is for White Americans, and that its false negative rate is lower for African-Americans than it is for White Americans.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc8df3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Facial recognition\n",
    "\n",
    "* The table below comes from [a paper](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) that analyzes several \"gender classifiers\", and shows that popular classifiers perform much worse for women and those with darker skin colors.\n",
    "* Police departments are beginning to [use these models](https://www.usatoday.com/story/tech/2018/07/09/orlando-police-decide-keep-testing-amazon-facial-recognition-program/768507002/) for surveillance.\n",
    "* Self-driving cars use similar models to recognize pedestrians!\n",
    "\n",
    "<center><img src=\"imgs/imgnet.jpeg\" width=60%></center>\n",
    "\n",
    "Note:\n",
    "\n",
    "$$PPV = \\text{precision} = \\frac{TP}{TP+FP},\\:\\:\\:\\:\\:\\: TPR = \\text{recall} = \\frac{TP}{TP + FN}, \\:\\:\\:\\:\\:\\: FPR = \\frac{FP}{FP+TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b776c77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does bias occur?\n",
    "\n",
    "Remember, our models learn patterns from the training data. Various sources of bias may be present within training data:\n",
    "* Training data may not be representative of the population.\n",
    "    * There may be fewer data points for minority groups, leading to poorer model performance.\n",
    "* The features chosen may be more useful in making predictions for certain groups than others.\n",
    "* Training data may encode existing human biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d254608",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gender associations\n",
    "\n",
    "- English is not a gendered language â€“ words like \"teacher\" and \"scientist\" are not inherently gendered (unlike in, say, French). \n",
    "- However, English does have gendered pronouns (e.g. \"he\", \"she\").\n",
    "- Humans subconsciously associate certain words with certain genders.\n",
    "- What gender does English associate the following words with?\n",
    "\n",
    "<center>soldier, teacher, nurse, doctor, dog, cat, president, nanny</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a474a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gender associations\n",
    "\n",
    "* Unlike English, Turkish ðŸ‡¹ðŸ‡· **does not** have gendered pronouns â€“ there is only a single, gender-neutral pronoun (\"o\").\n",
    "* Let's see what happens when we use Google Translate to translate Turkish sentences that **should be** gender-neutral back to English.\n",
    "* Click [this link](https://translate.google.com/?sl=tr&tl=en&text=o%20bir%20asker%20%0Ao%20bir%20Ã¶ÄŸretmen%0Ao%20bir%20mÃ¼hendis%0Ao%20bir%20hemÅŸire&op=translate) to follow along.\n",
    "* Why is this happening?\n",
    "    * Answer: Google Translate is \"trained\" on a large corpus of English text, and these associations are present in those English texts.\n",
    "    * Ideally, the results should contain a gender-neutral singular \"they\", rather than \"he\" or \"she\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af283372",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Image searches\n",
    "\n",
    "A 2015 study examined the image queries of vocations and the gender makeup in the search results. Since 2015, the behavior of Google Images has been improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceeabba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In 2015, a Google Images search for \"**nurse**\" returned...\n",
    "\n",
    "<center><img src='imgs/nurses2015.jpg'></center>\n",
    "\n",
    "Search for \"nurse\" now, what do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efac34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In 2015, a Google Images search for \"**doctor**\" returned...\n",
    "\n",
    "<center><img src='imgs/doctors2015.jpg'></center>\n",
    "\n",
    "Search for \"doctor\" now, what do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e2d91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ethics: What gender ratio _should_ we expect in the results?\n",
    "\n",
    "- Should it be 50/50?\n",
    "- Should it reflect the true gender distribution of those jobs?\n",
    "- More generally, what do you expect from your search results?\n",
    "    - This is a philosophical and ethical question, but one that **we need to think about as data scientists**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e2681",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='imgs/google-photos-paper.png' width=70%></center>\n",
    "\n",
    "Excerpts:\n",
    "\n",
    "> \"male-dominated professions tend to have even more men\n",
    "in their results than would be expected if the proportions\n",
    "reflected real-world distributions.\n",
    "\n",
    "> \"Peopleâ€™s existing perceptions of gender ratios in occupations\n",
    "are quite accurate, but that manipulated search results have an effect on perceptions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9199af3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How did this unequal representation occur?\n",
    "\n",
    "* The training data that Google Images searches from encoded existing biases.\n",
    "    - While 60% of doctors may be male, 80% of photos (including stock photos) of doctors on the internet may be of male doctors.\n",
    "* Models (like PageRank) that \"rank\" images find the, say, 5 \"most relevant\" image, not the 5 \"most typical\" images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95f2c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3375577",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notation \n",
    "\n",
    "* $C$ is a binary classifier.\n",
    "    * $C \\in \\{0, 1\\}$ is the prediction that the classifier makes.\n",
    "    * For instance, $C$ may predict whether or not an assignment is plagiarized.\n",
    "* $Y \\in \\{0,1\\}$ is the \"true\" label.\n",
    "* $A \\in \\{0, 1\\}$ is a binary attribute of interest.\n",
    "    * For instance, $A = 1$ may mean that you are a data science major, and $A = 0$ may mean that you are not a data science major."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19738f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea:** A classifier $C$ is \"fair\" if it performs the same for individuals in group $A$ and individuals outside of group $A$.\n",
    "    - But what do we mean by \"the same\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653faa7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demographic parity\n",
    "\n",
    "* A classifier $C$ achieves **demographic parity** if the proportion of the population for which $C = 1$ is the same both within A and outside A.\n",
    "$$\\mathbb{P}(C=1|A=1) = \\mathbb{P}(C=1|A\\neq 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30619dc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The assumption of demographic parity: the proportion of times the classifier predicts 1 is **independent** of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9eb61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Example 1:** $C$ is a binary classifier that predicts whether or not an essay is plagiarized.\n",
    "    - Suppose $A$ is \"class is a science class\".\n",
    "    - If $C$ achieves demographic parity, then **the proportion of the population for which an assignment is predicted to be plagiarized should be equal for science and non-science classes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281720b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Example 2:** $C$ is a binary classifier that predicts whether an image is of a doctor.\n",
    "    - Suppose $A$ is \"image is of a woman\".\n",
    "    - If $C$ achieves demographic parity, then **the proportion of the population for which the classification is \"doctor\" should be the same for women and non-women**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab50a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy parity\n",
    "\n",
    "- Demographic parity is not the only notion of \"fairness!\"\n",
    "    - You might expect more instances of plagiarism in non-science classes than you would in science classes; demographic parity says this is unfair but it may not be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368cd041",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A classifier $C$ achieves **accuracy parity** if the proportion of predictions that are classified correctly is the same both within $A$ and outside of $A$.\n",
    "\n",
    "$$\\mathbb{P}(C=Y|A=1) = \\mathbb{P}(C=Y|A\\neq 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690e2cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The assumption of accuracy parity: the classifier's accuracy should be independent of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f661d2a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Example:** $C$ is a binary classifier that determines whether someone receives a loan.\n",
    "    - Suppose $A$ is \"age is less than 25\".\n",
    "    - If the classifier is correct, i.e. if $C = Y$, then either $C$ approves the loan and it is paid off, or $C$ denies the loan and it would have defaulted.\n",
    "    - If $C$ achieves accuracy parity, then the proportion of correctly classified loans should be the same for those under 25 and those over 25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcbb3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### True positive parity\n",
    "\n",
    "- A classifier $C$ achieves **true positive parity** if the proportion of actually positive individuals that are correctly classified is the same both within $A$ and outside of $A$. \n",
    "\n",
    "$$\\mathbb{P}(C=1|Y=1, A=1) = \\mathbb{P}(C=1|Y=1, A\\neq 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be70d49b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A more natural way to think of true positive parity is as **recall parity** â€“ if $C$ achieves true positive parity, its recall should be independent of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682aac54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other measures of parity\n",
    "\n",
    "- We've just scratched the surface with measures of parity. \n",
    "- Any evaluation metric for a binary classifier can lead to a parity measure â€“ a parity measure requires \"similar outcomes\" across groups.\n",
    "    - Precision parity.\n",
    "    - False positive parity.\n",
    "- **Note:** Many of these parity conditions are **impossible** to satisfy simultaneously!\n",
    "    - See DSC 167 for more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6e46d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Loan approval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133fdef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LendingClub\n",
    "\n",
    "LendingClub is a \"peer-to-peer lending company\"; they [used to publish](https://www.lendingclub.com/info/download-data.action) a dataset describing the loans that they approved (fortunately, we downloaded it while it was available).\n",
    "\n",
    "* `'tag'`: whether loan was repaid in full (1.0) or defaulted (0.0)\n",
    "* `'loan_amnt'`: amount of the loan in dollars\n",
    "* `'emp_length'`: number of years employed\n",
    "* `'home_ownership'`: whether borrower owns (1.0) or rents (0.0)\n",
    "* `'inq_last_6mths'`: number of credit inquiries in last six months\n",
    "* `'revol_bal'`: revolving balance on borrows accounts\n",
    "* `'age'`: age in years of the borrower (protected attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fcfa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.read_csv('data/loan_vars1.csv', index_col=0)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193ad90",
   "metadata": {},
   "source": [
    "The total amount of money loaned was over 5 billion dollars! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33148e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['loan_amnt'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe0f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038ba67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting `'tag'`\n",
    "\n",
    "Let's build a classifier that predicts whether or not a loan was paid in full. If we were a bank, we could use our trained classifier to determine whether to approve someone for a loan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccccaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb552ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = loans.drop('tag', axis=1)\n",
    "y = loans.tag\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c1ce4",
   "metadata": {},
   "source": [
    "Recall, a prediction of 1 means that we predict that the loan will be paid in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da802548",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78523b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbd62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d46754",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(clf, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89367ce0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Precision describes the **proportion of loans that were approved that would have been paid back**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fbde48",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738e246",
   "metadata": {},
   "source": [
    "If we subtract the precision from 1, we get the proportion of loans that were approved that **would not** have been paid back. This is known as the **false discovery rate**.\n",
    "\n",
    "$$\\frac{FP}{TP + FP} = 1 - \\text{precision}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8288253",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - metrics.precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920473c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Recall describes the **proportion of loans that would have been paid back that were actually approved**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e738e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f30a3a",
   "metadata": {},
   "source": [
    "If we subtract the recall from 1, we get the proportion of loans that would have been paid back that **were denied**. This is known as the **false negative rate**.\n",
    "\n",
    "$$\\frac{FN}{TP + FN} = 1 - \\text{recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - metrics.recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380f3d0",
   "metadata": {},
   "source": [
    "From both the perspective of the bank and the lendee, a high false negative rate is bad!\n",
    "- The bank left money on the table â€“ the lendee would have paid back the loan, but they weren't approved for a loan.\n",
    "- The lendee deserved the loan, but weren't given one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca9139",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### False negative rate by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = X_test\n",
    "results['age_bracket'] = results['age'].apply(lambda x: 5 * (x // 5 + 1))\n",
    "results['prediction'] = y_pred\n",
    "results['tag'] = y_test\n",
    "\n",
    "(\n",
    "    results\n",
    "    .groupby('age_bracket')\n",
    "    .apply(lambda x: 1 - metrics.recall_score(x['tag'], x['prediction']))\n",
    "    .plot(kind='bar', title='False Negative Rate by Age Group')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac6d9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computing parity measures\n",
    "\n",
    "- $C$: Our random forest classifier (1 if we approved the loan, 0 if we denied it).\n",
    "- $Y$: Whether or not they truly paid off the loan (1) or defaulted (0).\n",
    "- $A$: Whether or not they were under 25 (1 if under, 0 if above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['is_young'] = (results.age < 25).replace({True: 'young', False: 'old'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028b388",
   "metadata": {},
   "source": [
    "First, let's compute the proportion of loans that were approved in each group. If these two numbers are the same, $C$ achieves demographic parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bda10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby('is_young')['prediction'].mean().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd587f",
   "metadata": {},
   "source": [
    "$C$ evidently does not achieve demographic parity â€“ older people are approved for loans far more often! Note that this doesn't factor in whether they were _correctly_ approved or _incorrectly_ approved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea011ada",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's compute the accuracy of $C$ in each group. If these two numbers are the same, $C$ achieves accuracy parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    results\n",
    "    .groupby('is_young')\n",
    "    .apply(lambda x: metrics.accuracy_score(x['tag'], x['prediction']))\n",
    "    .rename('accuracy')\n",
    "    .to_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6cddc",
   "metadata": {},
   "source": [
    "Hmm... These numbers look much more similar than before!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4eb148",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is this difference in accuracy significant?\n",
    "\n",
    "Let's run a **permutation test** to see if the difference in accuracy is significant.\n",
    "- Null Hypothesis: The classifier's accuracy is the same for both young people and old people, and any differences are due to chance.\n",
    "- Alternative Hypothesis: The classifier's accuracy is higher for old people.\n",
    "- Test statistic: Difference in accuracy (young minus old).\n",
    "- Significance level: 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = results.groupby('is_young').apply(lambda x: metrics.accuracy_score(x['tag'], x['prediction'])).diff().iloc[-1]\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_in_acc = []\n",
    "for _ in range(100):\n",
    "    s = (\n",
    "        results[['is_young', 'prediction', 'tag']]\n",
    "        .assign(is_young=results.is_young.sample(frac=1.0, replace=False).reset_index(drop=True))\n",
    "        .groupby('is_young')\n",
    "        .apply(lambda x: metrics.accuracy_score(x['tag'], x['prediction']))\n",
    "        .diff()\n",
    "        .iloc[-1]\n",
    "    )\n",
    "    \n",
    "    diff_in_acc.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pd.Series(diff_in_acc).plot(kind='hist', histnorm='probability', nbins=20,\n",
    "                            title='Difference in Accuracy (Young - Old)')\n",
    "fig.add_vline(x=obs, line_color='red')\n",
    "fig.update_layout(xaxis_range=[-0.1, 0.05])\n",
    "fig.add_annotation(text='<span style=\"color:red\">Observed Difference in Accuracy</span>', x=-0.075,showarrow=False, y=0.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b781a",
   "metadata": {},
   "source": [
    "It seems like the difference in accuracy across the two groups **is significant**, despite being only ~6%. Thus, $C$ likely does not achieve accuracy parity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e001ad2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ethical questions of fairness\n",
    "\n",
    "- **Question:** Is it \"fair\" to deny loans to younger people at a higher rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41537df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One answer: yes!\n",
    "    - Young people default more often.\n",
    "    - To have same level of accuracy, we need to deny them loans more often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9593ddc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Other answer: no!\n",
    "    - Accuracy isn't everything.\n",
    "    - Younger people **need** loans to buy houses, pay for school, etc.\n",
    "    - The bank should be required to take on higher risk; this is the cost of operating in a society."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd7962",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Federal law prevents age from being used as a determining factor in denying a loan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c030f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not only should we use `'age'` to determine whether or not to approve a loan, but we also shouldn't use other features that are strongly correlated with `'age'`, like `'emp_length'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3874771",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parting thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76450a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/DSLC.png\" width=\"40%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac6622",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Course goals âœ…\n",
    "\n",
    "In this course, you...\n",
    "\n",
    "* **Practiced** translating potentially vague questions into quantitative questions about measurable observations.\n",
    "* **Learned** to reason about 'black-box' processes (e.g. complicated models).\n",
    "* **Understood** computational and statistical implications of working with data.\n",
    "* **Learned** to use real data tools (e.g. love the documentation!).\n",
    "* **Got** a taste of the \"life of a data scientist\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55659387",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Course outcomes âœ…\n",
    "\n",
    "Now, you...\n",
    "\n",
    "* Are **prepared** for internships and data science \"take home\" interviews!\n",
    "* Are **ready** to create your own portfolio of personal projects.\n",
    "* Have the **background** and **maturity** to succeed in the upper-division."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda5913",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topics covered âœ…\n",
    "\n",
    "We learnt a lot this quarter.\n",
    "\n",
    "- Week 1: From BabyPandas to Pandas\n",
    "- Week 2: DataFrames\n",
    "- Week 3: Messy Data\n",
    "- Week 4: Hypothesis and Permutation Testing\n",
    "- Week 5: Missingness Mechanisms and Imputation\n",
    "- Week 6: Web Scraping, **Midterm Exam**\n",
    "- Week 7: Text Data\n",
    "- Week 8: Feature Engineering, Modeling in `scikit-learn`\n",
    "- Week 9: Pipelines and Generalization\n",
    "- Week 10: Classifier Evaluation, Fairness Criteria\n",
    "- Week 11: **Final Exam**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c5af02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thank you!\n",
    "\n",
    "- This course would not have been possible without our 11 tutors: Nicole Brye, John Driscoll, Doris Gao, Yuxin Guo, Daniel Li, Weiyue Li, Karthikeya Manchala, Yash Potdar, Ethan Shapiro, Costin Smilovici, and Tiffany Yu.\n",
    "\n",
    "- Don't be a stranger â€“ our contact information is at [dsc80.com/staff](https://dsc80.com/staff)!\n",
    "\n",
    "    - This quarter's course website will remain online permanently at [dsc-courses.github.io](https://dsc-courses.github.io).\n",
    "\n",
    "- **Apply to be a tutor in the future!** [Learn more here](https://datascience.ucsd.edu/academics/undergraduate/dsc-tutors/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72ed79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h3>Good luck on the Final Exam, and enjoy your summer break! ðŸŽ‰</h3></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
